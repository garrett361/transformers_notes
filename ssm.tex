\part{State Space Models}

\section{Intro\label{sec_ssm_intro}}

Needing to re-reference the entire previously-generated prefix at generation time is a major pain
point for transformers models. Token generation is $ \Ocal \left( S \right)  $ State space models
return, more or less, to the old LSTM type strategy of encoding the conditional history which
informs generation into a finite-sized state. The dream is faster generation and better memory
efficiency.


\section{S4 \label{sec_s4}}

The S4 model of \cite{s4} is a good starting point.  These are based off a continuous representation
in which some input signal\footnote{We use the notation of the mamba paper \cite{mamba}, which
differs from that of the S4 paper \cite{s4}.} $ x _{ a }(t) $ is converted to an output $ y _{ c
}(t) $ via an in
intermediate latent variable $ h _{ b }(t) $, with the above related as in
\begin{align}
    \partial _{ t }h _{ b }(t) &= A _{ b b' }h _{ b' }(t) + B _{ b a }x _{ a }(t)\nn
    y _{ c }(t) &= C _{ h b }h _{ b }(t) + D _{ c a }x _{ a }(t) \label{eq_s4_continuous} \ .
\end{align}
The capitalized tensors are the learnable weight matrices. $ D $ is often set to zero in the
literature. Basically, the information in the sequence $ x _{ s }  $ is stored in $ h _{ s }  $, an
internal memory for the model, much like the RNN/LSTM models of the past.

For discrete sequences, we discretize:
\begin{align}
    h  _{ bs } &= A  _{ b b' }h _{ b' (s-1) } + B  _{ b a }x  _{ as }\nn
    y  _{ cs } &= C _{ c b }h  _{ bs } + D _{ c a }x  _{ as } \label{eq_s4_discrete} \ .
\end{align}
where one can also relate these weights to those in \eqref{eq_s4_continuous} given the
discretization scheme (see \ref{sec_mamba}).

Subject to the initial condition $ h _{ b } ^{ -1 } =0 $, the above solves to
\begin{align}
    y _{ s }   &= \sum _{ s'=0 } ^{ s }  C \cdot  A ^{ s - s } \cdot B \cdot  x _{ s' } + D x _{ s }  \ , \label{eq_s4_soln}
\end{align}
omitting hidden dimension indices. Proper normalization of the various weights is non-trivial; see
\cite{s4} for details. Further, diagonalization clearly makes the $ A ^{ s-n } $ computation easier,
but care must be taken here, too. Clearly, the above computation is highly parallelizable. The S4
(and mamba) papers describe \eqref{eq_s4_soln} as a

Writing the above operation as $ y _{cs }   = \Sigma _{ca s s' } x _{ as' } $, one can build an
non-linear S4 layer by acting on the output with a non-linearity and then mixing feature dimensions
with a weight matrix:
\begin{align}
     z  _{ cs }  &= W _{ c c' } \phi \left ( \Sigma ^{c'a s s' } x _{ as' } \right ) \label{eq_s4_layer}
\end{align}
Assuming the $ c  $ and $ a  $ hidden dimensions have the same size, the operations can then be
naturally composed.

Taking all hidden dimensions to have size $ \Ocal \left( D \right)  $, the number of learnable
weights is $ \Ocal \left( D ^{ 2 }  \right)  $. Training can be parallelized across the sequence
dimension (via the representation \eqref{eq_s4_soln}, scaling linearly in sequence length. Iterative
generation from $ x _{ as } \longrightarrow y _{ cs }  $, given knowledge of the previous hidden
state $ h _{ b (s-1) }  $ takes only $ \Ocal \left( D ^{ 2 } \right)  $ (via the representation
\eqref{eq_s4_discrete}). There is no sequence-length dependence for next-output generation, unlike
for transformers, which is the main draw here: constant-time generation.

\section{Mamba\label{sec_mamba}}

A large limitation of the S4 model \eqref{eq_s4_discrete} is that the various weights are fixed
quantities which do not adjust to the input\footnote{For instance, we could ask our architecture to
    process two independent sequences concatenated together with a special separator token in the
    middle. The hidden state should be reset at the separator token and the mamba architecture would
be (in-principle) capable of this, while the S4 would not.} $ x _{ sd }  $. Mamba \cite{mamba}
extends S4 by replacing the fixed weights by functions of the inputs. This destroys the recursive
structure and requires various techniques for an efficient GPU implementation, which is the primary
focus of the paper.

The mamba architecture is as follows, based on the implementation in
\href{https://github.com/alxndrTL/mamba.py}{\texttt{mamba.py}} and
\href{https://github.com/state-spaces/mamba}{\texttt{mamba\_ssm}}.  Notation for dimensions and
tensors:
\begin{itemize}
    \item Mamba maps sequences to sequences, the same as for transformers. $ z _{ sd } =
        \texttt{mamba}\left ( x _{ sd } \right ) $. Batch dimension suppressed throughout.
    \item Various dimensions:
        \begin{itemize}
            \item $ d\in \left \{ 0, \ldots , D-1 \right \} $: the input's hidden dimensions,
                \texttt{d\_model}.
            \item $ e\in \left \{ 0, \ldots , E\times D-1 \right \} $: expanded internal hidden
                dimension. Usually $ E=2 $ in practice.
            \item $ s\in \left \{ 0, \ldots , S-1 \right \} $: sequence length.
            \item $ n\in \left \{ 0, \ldots , N-1 \right \} $: another internal hidden dimension,
                controlling the size of the internal memory; \texttt{d\_state}. Defaults to 16.
            \item $ r\in \left \{ 0, \ldots , R-1 \right \} $: another internal hidden dimension,
                \texttt{d\_rank}. Defaults to $ \lceil D /16 \rceil $.
            \item $ c\in \left \{ 0, \ldots , C-1 \right \} $: convolution kernel size;
                \texttt{d\_conv}, 4 by default. Used to convolve over the sequence dimension.
        \end{itemize}
    \item Learnable parameters\footnote{In practice, many of these are fused together for more
        efficient matmuls. We also omit potential bias terms.}:
        \begin{itemize}
            \item Two in-projectors from \texttt{d\_model } to the expanded dimension: $ W ^{ I _{ 0 } } _{ ed } $, $ W ^{ I _{ 1 } } _{ ed } $.
            \item Out-projector from the expanded internal dimension back to \texttt{d\_model } $ W ^{ O } _{ de } $.
            \item Two projectors used in creating the intermediate $ \Delta _{ se } $: $ W ^{ \Delta _{ 0 } }_{ re } $, $ W ^{ \Delta _{ 1 } }_{ er } $.
            \item Projectors for creating the intermediates $ B _{ sn } $ and $ C _{ sn } $: $ W ^{
                B } _{ ne } $, $ W ^{ C } _{ ne } $
            \item Convolutional kernel $ W ^{ K }_{ ec } $.
            \item Selective-scan weights $ W ^{ A } _{ en } $.
            \item Residual connection weights $ W ^{ D } _{ e } $.
        \end{itemize}
\end{itemize}
The notation here is not the same as that of the papers. We write all learnable weights as $ W ^{ X }_{ \ldots  } $.


Mamba blocks then perform the following logical operation:
\begin{algo}{Mamba}
\State \textbf{Inputs}: tensor $ x _{ sd }\in \mathbb{R}^{ S\times D } $
\State $ x ^{ 0 } _{ se } = W ^{ I _{ 0 } }_{ ed } x _{ sd }$, $ x ^{ 1 }  _{ se } = W ^{ I _{ 1 } }_{ ed } x _{ sd } $ \Comment Create expanded tensors from inputs (can fuse)
\State $ x ^{ 2 } _{ se } = K _{ ess' }\star x ^{ 1 }_{ se }$ \Comment 1D grouped convolution over the sequence dimension using $ W ^{ K }_{ ec } $.
\State $ x ^{ 3 } _{ se } = \phi \left ( x ^{ 2 }_{ se } \right )$ \Comment Elementwise non-linearity (\texttt{F.silu} default)
\State $ x ^{ 4 } _{ se } = \texttt{selective\_scan} \left ( x ^{ 3 }_{ se } \right )$ \Comment Selective scan (see below).
\State $ x ^{ 5 } _{ se } = x ^{ 4 } _{ se }\otimes\phi \left ( x ^{ 0 }_{ se } \right )$ \Comment Elementwise product and non-linearity (\texttt{F.silu} default)
\State $ z _{ sd } = W ^{ O }_{ de }x ^{ 5 }_{ se }$ \Comment Project back down.
\State \Return $ z _{ sd } \in \mathbb{R}^{ S\times D }$
\label{algo_mamba_1}
\end{algo}

where \texttt{selective\_scan} operation is the above is\footnote{The \texttt{mamba\_ssm} and
\texttt{mamba.py} implementations differ in the first step in that the latter optionally applies a
norm operator post-projection. The exponentials here might seem odd, but are probably motivated by
the existence of good cumulative sum kernels, which is how the exponents can be computed.}

\begin{algo}{Selective Scan: \texttt{selective\_scan} }
\State \textbf{Inputs}: tensor $ x _{ se }\in \mathbb{R}^{ S\times E } $
\State $ B _{ sn } = W ^{ B }_{ ne }x _{ se } $  \Comment Create intermediates $B, C, \Delta  $ (can
fuse).
\State $ C _{ sn } = W ^{ C }_{ e }x _{ se } $
\State $ \Delta  _{ se } = W ^{ \Delta  _{ 1 } }_{ er } W ^{ \Delta _{ 0 } } _{ re } x _{ se } $.
\State Solve recursion, subject to $ h _{ (-1)en }=0 $:
\begin{align}
    h _{ sen } &= \exp \left ( \Delta  _{ se } W^{A} _{ en } \right ) h _{ (s-1)e n} + \Delta _{ se }B _{ sn }x _{ se }\nn
    y _{ se } &= C _{ sn }h _{ sen } + W ^{ D } _{ e }x _{ se } \nn
    \implies y _{ se } &= C _{ sn }\left (\sum _{ s'=0 }^{ s }e^{ \Delta _{ se }W^{A} _{ en } }\times \ldots \times e^{ \Delta _{ s'e }W^{A} _{ en } } \Delta _{ s'e } B _{ s'n } x _{ s'e }\right ) + W ^{ D } _{ e } x _{ se }
\end{align}
\State \Return $ y _{ se } \in \mathbb{R}^{ S\times E }$
\label{algo_mamba1_scan}
\end{algo}
As noted above, the creation of the intermediates $ x ^{ 0 }_{ se }, x ^{ 1 }_{ se }, B _{ sn }, C
_{ sn } $ and part of $ \Delta  _{ se } $ can all be formed in a single large matmul.

\subsection{Mamba 2}

Mamba2 introduces some changes:
\begin{itemize}
    \item The $ n $-dimension is expanded to \texttt{ngroups} such dimensions (though
        \texttt{ngroups}=1 is the default), with associated index $ g\in \left \{ 0, \ldots , G-1
        \right \} $, $ G \equiv  \texttt{ngroups} $. Adding a non-trivial \texttt{ngroups} seems
        completely degenerate with expanding the $ n $ dimension of size \texttt{d\_state} to size
        $\texttt{d\_state}\times \texttt{ngroups} $.
    \item A head-index $ a \in \left \{ 0, \ldots , A-1 \right \} $ ($ A \equiv  \texttt{nheads} $)
        and head dimension $ h \in \left \{ 0, \ldots , H \right \} $ ($ A\times H= E $) are
        introduced, analogously to transformers.
    \item The $ e $-index from two selective-scan weights is removed: they are now per-head scalars $
        W ^{ A }_{ a }, W ^{ D }_{ a } $.
    \item The intermediate $ \Delta _{ sa } $ is also reduced to a per-head, per-sequence-position
        scalar, with respect to the hidden dimension. This tensor is now created via a single matmul
        with weight $ W ^{ \Delta }_{ ae } $.
    \item The short 1D convolution is now also taken over the $ B $ and $ C $ intermediates with
        kernels $W ^{ K _{ B } }_{ gnc } $, $ W ^{ K _{ B } }_{ gnc } $.
\end{itemize}

The updated model:
\begin{algo}{Mamba2}
\State \textbf{Inputs}: tensor $ x _{ sd }\in \mathbb{R}^{ S\times D } $
\State $ x ^{ 0 } _{ se } = W ^{ I _{ 0 } }_{ ed } x _{ sd }$, $ x ^{ 1 }  _{ se } = W ^{ I _{ 1 } }_{ ed } x _{ sd } $ \Comment Create expanded tensors from inputs (can fuse)
\State $ x ^{ 2 } _{ se } = K _{ ess' }\star x ^{ 1 }_{ se }$ \Comment 1D grouped convolution over the sequence dimension (fused)
\State $ x ^{ 3 } _{ se } = \phi \left ( x ^{ 2 }_{ se } \right )$ \Comment Elementwise non-linearity (\texttt{F.silu} default)
\State $ x ^{ 4 } _{ se } = \texttt{selective\_scan2} \left ( x ^{ 3 }_{ se } \right )$ \Comment Selective scan (see below).
\State $ x ^{ 5 } _{ se } = {\rm Norm }\left (x ^{ 4 }  _{ se }\otimes\phi \left ( x ^{ 0 }_{ se } \right )\right ) _{ e }$ \Comment Elementwise product, non-linearity, and norm (\texttt{RMS} default)
\State $ z _{ sd } = W ^{ O }_{ de }x ^{ 5 }_{ se }$ \Comment Project back down.
\State \Return $ z _{ sd } \in \mathbb{R}^{ S\times D }$
\label{algo_mamba_2}
\end{algo}

The mechanical differences are the normalization step and the details of the
\texttt{selective\_scan2} operation, which is essentially the same as before, but now the hidden $ e
$ is split into multiple attention heads, analogously to transformer models:

\begin{algo}{Selective Scan 2: \texttt{selective\_scan2} }
\State \textbf{Inputs}: tensor $ x _{ se }\in \mathbb{R}^{ S\times E } $
\State $ x _{ sah } = x _{ s(ah) } $  \Comment Break the inputs up into attention heads.
\State $ B _{ sgn } = W ^{ B }_{ gne }x _{ se } $  \Comment Create intermediates $B, C, \Delta  $ (can
fuse)\footnote{The \texttt{mamba\_ssm} and \texttt{mamba.py} implementations differ here in that the
latter optionally applies a norm operator post-projection.}.
\State $ C _{ sgn } = W ^{ C }_{ gne }x _{ se } $
\State $ \Delta  _{ sa } = W ^{ \Delta   }_{ ae } x _{ se } $.
\State $ \Delta  _{ sa } = \texttt{Softplus} \left (\Delta _{ sa }\right ) $. \Comment For some reason. $ \texttt{Softplus}(x) \equiv \ln \left ( 1+ e^{ x } \right ) $.
\State $ B _{ sgn } = K ^{ B } _{ gnss' }\star B_{ sgn}$ \Comment 1D grouped convolution over the sequence dimension (fused)
\State $ C _{ sgn } = K ^{ C } _{ gnss' }\star C_{ sgn }$
\State Solve recursion, subject to $ h _{ (-1)gahn }=0 $:
\begin{align}
    h _{ sgahn } &= \exp \left ( \Delta  _{ sa } W^{A} _{ a } \right ) h _{ (s-1)gah n} + \Delta _{ sa }B _{ sgn }x _{ se }\nn
    y _{ sah } &= C _{ sgn }h _{ sgahn } +  W ^{ D } _{ a }x _{ sah } \nn
    \implies y _{ sah } &= C _{ sgn }\left (\sum _{ s'=0 }^{ s }e^{ \Delta _{ sa }W^{A} _{ a } }\times \ldots \times e^{ \Delta _{ s'a }W^{A} _{ a } } \Delta _{ s'a }B _{ s'gn } x _{ s'ah }\right ) + W ^{ D } _{ a } x _{ sah }
\end{align}
\State $ y _{ se } = y _{ s(ah) } $  \Comment Concatenate the heads back together.
\State \Return $ y _{ se } \in \mathbb{R}^{ S\times E }$
\label{algo_mamba2_scan}
\end{algo}
As before, many of the matmuls can be performed as one big operation, and the three short
convolutions can be similarly fused into a single convolution. The two algorithms
Algo.~\ref{algo_mamba1_scan} and Algo.~\ref{algo_mamba2_scan} are nearly identical; they just differ
in some tensor shapes.



\subsection{Mamba2 Duality with Attention}

There are only two steps in which tokens at different temporal positions interact in the Mamba2
model:
\begin{enumerate}
    \item In the short 1D convolution.
    \item In the recurrence relation, where we create the intermediate
        \begin{align}
        z _{ sah } &= C _{ sgn }\left (\sum _{ s'=0 }^{ s }e^{ \Delta _{ sa }W^{A} _{ a } }\times \ldots \times e^{ \Delta _{ s'a }W^{A} _{ a } } \Delta _{ s'a }B _{ s'gn } x _{ s'ah }\right ) \equiv  M  _{ ass' }x _{ s'ah }
        \end{align}
        which is the most complicated step of the model.
\end{enumerate}

As noted above, the second case is ultimately just a matrix-multiply on the input tensors $ x _{ sah
} $ with the tensor $ M _{ ass' } $, where operations across attention head are all independent. The
$ M _{ ass' } $ tensor has $ \Ocal \left( A S ^{ 2 } \right)  $ elements, which we clearly do not want
to concurrently materalize. All of this should sound familiar: the above is exactly analogous to the
structure and problems of flash attention, Sec.~\ref{subsec_flash_attention}, the only difference begin how the linear operator $ M _{
ass' } $ is constructed: $ M _{ ass' } = \Sm \left (q ^{ a }_{ sh }k ^{ a }_{ s'h } \right ) $
in standard attention, and as above for Mamba2, say. This is the ``duality" discussed in
\cite{dao2024transformersssmsgeneralizedmodels} and the broad strokes of the efficient algorithm
implementation for Mamba2 echos that of flash attention: partition the computation over the sequence
dimensions and compute $ z _{ sah } $ in chunks over the $ s $-dimension, so as to avoid realizing
any $ \Ocal \left( S ^{ 2 } \right)  $ tensors.

Similar statements hold for the original Mamba; the index names and choices just make the analogy
more readily recognizable in Mamba2.

\subsection{Aren't These Just RNNs?\label{subsec_rnns_and_ssm}}

Yes, but very special ones with the important computational difference that the recursion relations
are \textit{linear} in the hidden state $ h $. This crucial difference makes it possible to
parallelize the operations during training. Compare \eqref{eq_s4_discrete} to what typical RNN
recursion relations would look like:
\begin{align}
    h  _{ bs } &= \phi \left (A  _{ b b' }h _{ b' (s-1) } + B  _{ b a }x  _{ as }\right )\nn
    y  _{ cs } &= \phi\left (C _{ c b }h  _{ bs } + D _{ c a }x  _{ as }\right ) \label{eq_rnn_comparison} \ .
\end{align}
for some non-linearity $ \phi $.  The recursion relations would solve to an expression with nested
$ \phi $ factors which would make the computation of $ h _{ bs } $ non-associative. But in the linear $
\phi(x) = x$ limit, the operations are \textit{associative} which makes them
\textit{parallelizable}, via known scan algorithms \cite{prefixSumsBlelloch}.





