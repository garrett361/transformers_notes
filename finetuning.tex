\part{Fine Tuning}

\section{Instruction Fine Tuning}

Generally, instruction fine-tuning is a follow-on step after model pre-training\footnote{A
terminology note: pre-training is standard next-token training on an enormous, general dataset, supervised
fine-tuning typically indicates additional, subsequent training on a higher-quality, maybe
domain-specific dataset, and instruction fine-tuning follows.}.
The pre-training, pure next-token prediction task is altered to optimize an objective which now
incorporates other data, typically information regarding human preferences\footnote{One failure mode
this corrects for: next-token training would do best by replicating common mistakes in grammar or
statements of fact which can be corrected for using these methods.}.


\subsection{Direct Preference Optimization}

Direct Preference Optimization (DPO) \cite{rafailov2024directpreferenceoptimizationlanguage} is a
vast simplification of previous reinforcement-learning based methods (namely PPO-based ones
\cite{schulman2017proximalpolicyoptimizationalgorithms}).


DPO aims to solve the RLHF optimization problem defined over a dataset $\Dcal \sim  (x, y _{ l }, y
_{ w }) $ corresponding to prefixes ($ x $) and pairs of preferred and dis-preferred
completions\footnote{I guess the $ l, w $ subscripts are for "lose" and "win"?} ($ y _{ l }, y _{ w
} $). The relevant components are:
\begin{enumerate}
    \item A baseline language model: $ \pi _{ {\rm  ref} } (y|x)$, usually a supervised fine-tuned
        model trained on high-quality data.
    \item The to-be-trained model: $ \pi _{\theta } (y|x)$, usually initialized to $ \pi _{ {\rm
        ref} } (y|x)$. This is the \textit{policy} in the literature.
    \item A reward model which produces $ p(y _{ w } \succ y _{ l }| x ) $, the
        probability\footnote{Whether one completion is preferred over another is a probabalistic
        question since, e.g., not everyone in the population will agree.} $ y _{ w } $ is favored
        over $ y _{ l } $.  The reward function $ r(x, y) $ reflects how well $ y $ completes the
        prefix $ x $, in this context, and we assume the probability can be expressed in terms of
        the reward function $ p(y _{ w } \succ y _{ l }| x ) = p( r(x, y _{ w }), r(x, y _{ l }) )
        $. The reward model is commonly an LLM with a scalar output head attached.
\end{enumerate}


First, a quick review of RLHF, which proceeds in stages. First, $ \Dcal $ is used to train a reward
model informed by the dataset $ \Dcal $. The optimal reward model $ r_{ \star } $ minimizes the
binary cross-entropy loss over $ \Dcal $, which is just
\begin{align}
  \Lcal _{ r }   &= -E _{ x, y _{ l }, y _{ w } \sim \Dcal } \ln p(y _{ w } \succ y _{ l }| x ) \ . \label{eq_rlhf_reward_loss}
\end{align}
 The reward model embodies human preferences and we want to
transfer this knowledge to the language model $ \pi _{ \theta } $. This can be done by optimizing $
\pi _{ \theta  } $ to generate completions of inputs that lead to large rewards, reflecting
human-preferred generations. In order to also keep the model from straying too far from its
reference base, a tunable KL-divergence penalty is also added\footnote{We've written the above as a
loss so that we're minimizing everywhere.}:
\begin{align}
    \Lcal _{ {\rm RLHF} }  &= E _{x \sim \Dcal, y \sim \pi _{ \theta  }(y|x) } \left ( -r _{ \star }
    (x, y) + \beta D _{ {\rm KL} } \left ( \pi  _{ \theta  }(y|x)|| \pi _{{\rm ref}}(y|x) \right )
\right ) \ . \label{eq_rlhf_loss}
\end{align}
 Reinforcement-learning methods are typically used to optimize the $ \pi _{ \theta  } $ model and
 the generation step is particularly costly.  In particular, the usual gradient-based optimization
 methods cannot be used because the loss depends on generated tokens which are discontinuous
 (non-differentiable) functions of the model's parameters.

DPO improves upon RLHF by skipping any generation step, removing the explicit reward function, and
making the optimization problem amenable to gradient based methods by choosing a specific functional
relation between the reward function $ r(x, y) $ and the preference probability $p(y _{ w } \succ y
_{ l }| x )$. Whereas RLHF minimizes the loss $ \Lcal _{ {\rm rlhf} } $ \eqref{eq_rlhf_loss} subject
to a fixed, optimal reward function found by first minimizing the reward loss $ \Lcal _{ r } $
\eqref{eq_rlhf_reward_loss}, DPO is essentially derived in the opposite direction: first, find the
functional form of $ \pi _{ \theta } $ which minimizes the RLHF loss for an arbitrary reward
function, and then use this form when minimizing of the cross-entropy defining the reward
function\footnote{This is analogous to minimizing the regular function $f(x, y)$ subject to also
    minimizing $ g(x) $. This can either be done by solving the second for $ x _{ \star } $ and
    minimizing $ f(x _{ \star }, y) $ (the RLHF strategy), or first solving $ \frac{ \partial f }{
\partial y  } =0$ to find $ x _{ \star }(y) $ and then minimizing $ g(x _{ \star }(y)) $ (the DPO
strategy).}.



The $ \pi _{ \theta  } $ which minimizes the RLHF loss \eqref{eq_rlhf_loss} for
an arbitrary reward function $ r(x, y) $ is given by\footnote{This is easy to show using the
calculus of variations, though it's not the route taken in the paper. The explicit RLHF loss is $
\Lcal _{ {\rm RLHF} } = \int \rd x\,\rd y\,p(x) \pi _{ \theta  }(y|x)\left ( -r(x,y) +\beta \ln \pi _{
\theta }(y|x)\ /\pi _{ {\rm ref} }(y|x) \right )  $ and we want to minimize this subject to the
constraint that $ \pi _{ \theta  }(y|x) $ is properly normalized. So, we use a Lagrange multiplier
and extremize $ \Lcal'  = \Lcal _{ {\rm RLHF} }+ \int \rd x\,\rd y\, \lambda (x) \pi _{ \theta
}(y|x)   $. Solving $ \frac{ \delta \Lcal ' }{ \delta \pi _{ \theta  } (y|x)} =0$ yields
\eqref{eq_dpo_soln}.
}
\begin{align}
\pi _{\theta}(y|x) &= \frac{ \pi _{ {\rm ref}} (y|x)e ^{ r(x, y)/ \beta  }  }{ Z(x) } \ ,\label{eq_dpo_soln}
\end{align}
where $ Z(x) = \int \rd y\, \pi _{ {\rm ref} }(y|x)e^{ r(x, y)/ \beta  }$ is a intractable
normalization (partition function) factor. However, if  $p(y _{ w } \succ y _{ l }| x )$ only
depends on $ r(x, y _{ w }) $ and $ r(x, y _{ l }) $ through their difference, these factors cancel
out. Letting $ p(y _{ w } \succ y _{ l }| x ) = \sigma ( r(x, y _{ w })- r(x, y _{ l }) ) $, for
some\footnote{In the specific case where $ \sigma  $ is the sigmoid function, this is known as the
Bradley-Terry model.} $ \sigma  $, and eliminating the reward function in the cross-entropy loss via
\eqref{eq_dpo_soln} reduces $ \Lcal _{ r } $ to
\begin{align}
    \Lcal _{ {\rm DPO} } &= -E _{ x, y _{ l }, y _{ w } \sim \Dcal } \ln \sigma \left (\beta \left (\ln \frac{ \pi _{
\theta }(y_{ w }|x ) }{ \pi _{ {\rm ref} }(y_{ w }|x ) }-\ln \frac{ \pi _{ \theta }(y _{ l }|x) }{ \pi
_{ {\rm ref} }(y _{  l}|x) }  \right )\right ) \ , \label{eq_dpo_reward_loss}
\end{align}
which we've now renamed the DPO loss.   The loss \eqref{eq_dpo_reward_loss} can now be minimized by
standard, gradient based methods without any generation step.





