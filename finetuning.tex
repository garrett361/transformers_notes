\part{Fine Tuning}

\section{Instruction Fine Tuning}

Generally, instruction fine-tuning is a follow-on step after model pre-training\footnote{A
terminology note: pre-training is standard next-token training on an enormous, general dataset, supervised
fine-tuning typically indicates additional, subsequent training on a higher-quality, maybe
domain-specific dataset, and instruction fine-tuning follows.}.
The pre-training, pure next-token prediction task is altered to optimize an objective which now
incorporates other data, typically information regarding human preferences\footnote{One failure mode
this corrects for: next-token training would do best by replicating common mistakes in grammar or
statements of fact which can be corrected for using these methods.}.


\subsection{Direct Preference Optimization \label{subsec_dpo}}

Direct Preference Optimization (DPO) \cite{rafailov2024directpreferenceoptimizationlanguage} is a
vast simplification of previous reinforcement-learning based methods (namely PPO-based ones
\cite{schulman2017proximalpolicyoptimizationalgorithms}).


DPO aims to solve the RLHF optimization problem defined over a dataset $\Dcal \sim  (x, y _{ l }, y
_{ w }) $ corresponding to prefixes ($ x $) and pairs of preferred and dispreferred
completions\footnote{I guess the $ l, w $ subscripts are for "lose" and "win"?} ($ y _{ l }, y _{ w
} $). The relevant components are:
\begin{enumerate}
    \item A baseline language model: $ \pi _{ {\rm  ref} } (y|x)$, usually a supervised fine-tuned
        model trained on high-quality data.
    \item The to-be-trained model: $ \pi _{\theta } (y|x)$, usually initialized to $ \pi _{ {\rm
        ref} } (y|x)$. This is the \textit{policy} in the literature.
    \item A reward model which produces $ p(y _{ w } \succ y _{ l }| x ) $, the
        probability\footnote{Whether one completion is preferred over another is a probabalistic
        question since, e.g., not everyone in the population will agree.} $ y _{ w } $ is favored
        over $ y _{ l } $.  The reward function $ r(x, y) $ reflects how well $ y $ completes the
        prefix $ x $, in this context, and we assume the probability can be expressed in terms of
        the reward function $ p(y _{ w } \succ y _{ l }| x ) = p( r(x, y _{ w }), r(x, y _{ l }) )
        $. The reward model is commonly an LLM with a scalar output head attached.
\end{enumerate}


First, a quick review of RLHF, which proceeds in stages. First, $ \Dcal $ is used to train a reward
model informed by the dataset $ \Dcal $. The optimal reward model $ r_{ \star } $ minimizes the
binary cross-entropy loss over $ \Dcal $, which is just
\begin{align}
  \Lcal _{ r }   &= -E _{ x, y _{ l }, y _{ w } \sim \Dcal } \ln p(y _{ w } \succ y _{ l }| x ) \ . \label{eq_rlhf_reward_loss}
\end{align}
 The reward model embodies human preferences and we want to
transfer this knowledge to the language model $ \pi _{ \theta } $. This can be done by optimizing $
\pi _{ \theta  } $ to generate completions of inputs that lead to large rewards, reflecting
human-preferred generations. In order to also keep the model from straying too far from its
reference base, a tunable KL-divergence penalty is also added\footnote{We've written the above as a
loss so that we're minimizing everywhere.}:
\begin{align}
    \Lcal _{ {\rm RLHF} }  &= E _{x \sim \Dcal, y \sim \pi _{ \theta  }(y|x) } \left ( -r _{ \star }
    (x, y) + \beta D _{ {\rm KL} } \left ( \pi  _{ \theta  }(y|x)|| \pi _{{\rm ref}}(y|x) \right )
\right ) \ . \label{eq_rlhf_loss}
\end{align}
 Reinforcement-learning methods are typically used to optimize the $ \pi _{ \theta  } $ model and
 the generation step is particularly costly.  In particular, the usual gradient-based optimization
 methods cannot be used because the loss depends on generated tokens which are discontinuous
 (non-differentiable) functions of the model's parameters.

DPO improves upon RLHF by skipping any generation step, removing the explicit reward function, and
making the optimization problem amenable to gradient based methods by choosing a specific functional
relation between the reward function $ r(x, y) $ and the preference probability $p(y _{ w } \succ y
_{ l }| x )$. Whereas RLHF minimizes the loss $ \Lcal _{ {\rm rlhf} } $ \eqref{eq_rlhf_loss} subject
to a fixed, optimal reward function found by first minimizing the reward loss $ \Lcal _{ r } $
\eqref{eq_rlhf_reward_loss}, DPO is essentially derived in the opposite direction: first, find the
functional form of $ \pi _{ \theta } $ which minimizes the RLHF loss for an arbitrary reward
function, and then use this form when minimizing of the cross-entropy defining the reward
function\footnote{This is analogous to minimizing the regular function $f(x, y)$ subject to also
    minimizing $ g(x) $. This can either be done by solving the second for $ x _{ \star } $ and
    minimizing $ f(x _{ \star }, y) $ (the RLHF strategy), or first solving $ \frac{ \partial f }{
\partial y  } =0$ to find $ x _{ \star }(y) $ and then minimizing $ g(x _{ \star }(y)) $ (the DPO
strategy).}.



The $ \pi _{ \theta  } $ which minimizes the RLHF loss \eqref{eq_rlhf_loss} for
an arbitrary reward function $ r(x, y) $ is given by\footnote{This is easy to show using the
calculus of variations, though it's not the route taken in the paper. The explicit RLHF loss is $
\Lcal _{ {\rm RLHF} } = \int \rd x\,\rd y\,p(x) \pi _{ \theta  }(y|x)\left ( -r(x,y) +\beta \ln \pi _{
\theta }(y|x)\ /\pi _{ {\rm ref} }(y|x) \right )  $ and we want to minimize this subject to the
constraint that $ \pi _{ \theta  }(y|x) $ is properly normalized. So, we use a Lagrange multiplier
and extremize $ \Lcal'  = \Lcal _{ {\rm RLHF} }+ \int \rd x\,\rd y\, \lambda (x) \pi _{ \theta
}(y|x)   $. Solving $ \frac{ \delta \Lcal ' }{ \delta \pi _{ \theta  } (y|x)} =0$ yields
\eqref{eq_dpo_soln}.
}
\begin{align}
\pi _{\theta}(y|x) &= \frac{ \pi _{ {\rm ref}} (y|x)e ^{ r(x, y)/ \beta  }  }{ Z(x) } \ ,\label{eq_dpo_soln}
\end{align}
where $ Z(x) = \int \rd y\, \pi _{ {\rm ref} }(y|x)e^{ r(x, y)/ \beta  }$ is a intractable
normalization (partition function) factor. However, if  $p(y _{ w } \succ y _{ l }| x )$ only
depends on $ r(x, y _{ w }) $ and $ r(x, y _{ l }) $ through their difference\footnote{In
\cite{rafailov2024directpreferenceoptimizationlanguage}, the DPO symmetry $ r(x, y)\longrightarrow r(x,
y) + f(x) $, for arbitrary $ f(x) $, is said to induce an equivalence class relation between
different reward functions.}, these factors cancel
out. Letting $ p(y _{ w } \succ y _{ l }| x ) = \sigma ( r(x, y _{ w })- r(x, y _{ l }) ) $, for
some\footnote{In the specific case where $ \sigma  $ is the sigmoid function, this is known as the
Bradley-Terry model.} $ \sigma  $, and eliminating the reward function in the cross-entropy loss via
\eqref{eq_dpo_soln} reduces $ \Lcal _{ r } $ to
\begin{align}
    \Lcal _{ {\rm DPO} } &= -E _{ x, y _{ l }, y _{ w } \sim \Dcal } \ln \sigma \left (\beta \left (\ln \frac{ \pi _{
\theta }(y_{ w }|x ) }{ \pi _{ {\rm ref} }(y_{ w }|x ) }-\ln \frac{ \pi _{ \theta }(y _{ l }|x) }{ \pi
_{ {\rm ref} }(y _{  l}|x) }  \right )\right ) \ , \label{eq_dpo_reward_loss}
\end{align}
which we've now renamed the DPO loss.   The loss \eqref{eq_dpo_reward_loss} can now be minimized by
standard, gradient based methods without any generation step.



\subsection{KTO: Preference Finetuning without Pairs \label{subsec_kto}}

DPO requires a dataset of triplets: a prefix, one preferred completion, and one dispreferred
completion. KTO alignment \cite{ethayarajh2024ktomodelalignmentprospect} attempts to reduce the
inputs a prefix, a completion, and a binary signal indicating whether the output is desirable or
not, since such datasets are easier to construct.

The method is based on the ideas of Kahneman and Tversky and the central ingredient is a value
function which monotonically maps outcomes to perceived values $ v: \mathcal{Z}\longrightarrow
\mathbb{R}$, with $ \mathcal{Z} $ the space of outcomes. Some normalization point $ z _{ 0 } $
defines the boundary between positive and negative outcomes, the value function\footnote{Which can
be taken to satisfy $ v(0)=0 $.} is taken to be a function of $ z-z _{ 0 } $, and human value
functions are known to be convex for $ z > z _{ 0 }$ (diminishing returns) and exhibit loss
aversion\footnote{Which I suppose means that $v \left ( z-z_0 \right) + v \left ( z _{ 0 }-z \right)
\le 0 $ for $ z>0 $.}.

KTO applies this framework to the usual text-prediction problem as in the following. The space of
outcomes $ \mathcal{Z} $ is the reward function value taken to be
\begin{align}
    r _{ \theta  }(x, y)&\equiv \ln \frac{ \pi _{ \theta  }(y|x) }{ \pi _{ {\rm ref}}(y|x) } \ , \label{eq_kto_reward}
\end{align}
the difference in reference and model surprisal, as inspired by DPO. The reference point is just the
expected value of the reward function over prefixes and trainable-model-generated completions, i.e.,
the KL divergence averaged over prefixes:
\begin{align}
   z _{ 0 }  & \equiv  E _{ y \sim \pi _{ \theta  }(y|x)  ,   x \sim D  }r _{ \theta  }(x, y) =E _{ x \sim D  } D _{ {\rm KL} }(\pi _{ \theta  }(y|x)|| \pi _{ {\rm ref} }(y|x)) \ . \label{eq_kto_ref_pt}
\end{align}
Splitting the space of completions into desirable and undesirable ones,  $ \mathcal{Y}=
\mathcal{Y}_{ D }\cup \mathcal{Y}_{ U } $, the KTO loss\footnote{They also add a constant term to
    the loss for normalization purposes which we have omitted. The KTO loss falls into the broader
    category of Human Aware Loss Objectives (HALOs) which are a general class of objectives that
    roughly fit into the Kahneman-Tversky form. See the paper for a further discussion and
    comparison of HALO vs non-HALO methods.
} is taken to be:
\begin{align}
    \mathcal{L}_{ {\rm KTO} } &= - E _{ x, y \sim D }v(r _{ \theta  }(x, y) - z _{ 0 })\nn
    v(r _{ \theta  }(x, y)-z _{ 0 })&\equiv  \begin{cases}
        \lambda _{ D }\sigma \left ( \beta \left ( r _{ \theta  }(x, y) - z _{ 0 } \right ) \right ) & y \in \mathcal{Y}_{ D }\nn
        \lambda _{ U }\sigma \left ( \beta \left ( -r _{ \theta  }(x, y) + z _{ 0 } \right ) \right ) & y \in \mathcal{Y}_{ U } \label{eq_kto_loss}
    \end{cases}
\end{align}
for hyperparameters\footnote{Risk aversion would seem to require $ \lambda _{ U }> \lambda _{ D }$,
but the KTO paper empirically finds that the opposite regime performs better.} $ \beta , \lambda _{ D }, \lambda _{ U } \in
\mathbb{R} ^{ + } $ and where $ \sigma  $ is the sigmoid function. So,  $ v(r _{ \theta  }(x,
y)-z _{ 0 }) $ is maximized by sending $ r _{ \theta   } \longrightarrow  \infty $ for desirable
results and to $ -\infty $ for undesirable ones, while the normalization point $ z _{ 0 } $
concentrates updates on examples whose rewards do not stray wildly from the average reward, which
implicitly carries information about the reference model.

The reference point $ z _{ 0 } $ \eqref{eq_kto_ref_pt} is a problem, because it requires generation
which is both expensive and not differentiable (the problem DPO solves). So, the authors perform a
rough estimate of the scale and do not backpropagate through $ z _{ 0 } $, (which is a bit
questionable).

