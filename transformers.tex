\documentclass[11pt]{article}
%Importing custom commands
\usepackage{latex_goon/latex_goon}
\title{Transformers}
\author{Garrett Goon}
\begin{document}
%
%\maketitle

\vspace{1truecm}
%
%
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\begin{center}
{\huge \bf{Transformers}}
\end{center}


\begin{abstract}

Notes on various aspects of Transfomers

\end{abstract}

\tableofcontents


\renewcommand*{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{0}



\section{The Architecture}

The Transformers architecture \cite {vaswani2017attention}, which dominates Natural Language
Processing (NLP) as of July 2023, is a relatively simple architecture. There are various flavors and
variants of Tranformers, but we start by describing the decoder-only versions which underly the
GPT-X models \cite {gpt2radford2019language, gpt3brown2020language, gpt4openai2023}.






\section{Memory}



\begin{minted}{python}
  def hello_world(args):
    print("hello world this is a test")
\end{minted}


\appendix


\section{Conventions and Notation}\label{app:conventions}


We loosely follow the conventions of \cite{korthikanti2022reducing} and denote the main Transformers
parameters by:

\section{To-Do}\label{app:todo}


\bibliographystyle{latex_goon/utphys}
\bibliography{bibliography}
\end{document}
