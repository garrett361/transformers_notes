@article{korthikanti2022reducing,
    title = {Reducing Activation Recomputation in Large Transformer Models},
    author = {Vijay Korthikanti and Jared Casper and Sangkug Lym and Lawrence
              McAfee and Michael Andersch and Mohammad Shoeybi and Bryan
              Catanzaro},
    year = {2022},
    eprint = {2205.05198},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{vaswani2017attention,
    title = {Attention Is All You Need},
    author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob
              Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and
              Illia Polosukhin},
    year = {2017},
    eprint = {1706.03762},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}

@article{gpt2radford2019language,
    title = {Language models are unsupervised multitask learners},
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and
              Amodei, Dario and Sutskever, Ilya and others},
    journal = {OpenAI blog},
    volume = {1},
    number = {8},
    pages = {9},
    year = {2019},
}

@article{gpt3brown2020language,
    title = {Language Models are Few-Shot Learners},
    author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah
              and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and
              Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini
              Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom
              Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler
              and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark
              Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin
              Chess and Jack Clark and Christopher Berner and Sam McCandlish and
              Alec Radford and Ilya Sutskever and Dario Amodei},
    year = {2020},
    eprint = {2005.14165},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}


@article{gpt4openai2023,
    title = {GPT-4 Technical Report},
    author = {OpenAI},
    year = {2023},
    eprint = {2303.08774},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}

@article{ALiBi,
    author = {Ofir Press and Noah A. Smith and Mike Lewis},
    title = {Train Short, Test Long: Attention with Linear Biases Enables Input
             Length Extrapolation},
    journal = {CoRR},
    volume = {abs/2108.12409},
    year = {2021},
    url = {https://arxiv.org/abs/2108.12409},
    eprinttype = {arXiv},
    eprint = {2108.12409},
    timestamp = {Thu, 02 Sep 2021 14:42:29 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-2108-12409.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}

@article{xiong2020layer,
    title = {On Layer Normalization in the Transformer Architecture},
    author = {Ruibin Xiong and Yunchang Yang and Di He and Kai Zheng and Shuxin
              Zheng and Chen Xing and Huishuai Zhang and Yanyan Lan and Liwei
              Wang and Tie-Yan Liu},
    year = {2020},
    eprint = {2002.04745},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{yang2022tensor,
    title = {Tensor Programs V: Tuning Large Neural Networks via Zero-Shot
             Hyperparameter Transfer},
    author = {Greg Yang and Edward J. Hu and Igor Babuschkin and Szymon Sidor
              and Xiaodong Liu and David Farhi and Nick Ryder and Jakub Pachocki
              and Weizhu Chen and Jianfeng Gao},
    year = {2022},
    eprint = {2203.03466},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{dao2022flashattention,
    title = {FlashAttention: Fast and Memory-Efficient Exact Attention with
             IO-Awareness},
    author = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and
              Christopher RÃ©},
    year = {2022},
    eprint = {2205.14135},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}
@article{dao2023flashattention2,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Tri Dao},
  year={2023},
  eprint={2307.08691},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
@article{micikevicius2018mixed,
title = {Mixed Precision Training},
author = {Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory
          Diamos and Erich Elsen and David Garcia and Boris Ginsburg and
          Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao
          Wu},
year = {2018},
eprint = {1710.03740},
archivePrefix = {arXiv},
primaryClass = {cs.AI},
}
@article{shazeer2019fast,
title = {Fast Transformer Decoding: One Write-Head is All You Need},
author = {Noam Shazeer},
year = {2019},
eprint = {1911.02150},
archivePrefix = {arXiv},
primaryClass = {cs.NE},
}

@article{shoeybi2020megatronlm,
    title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using
             Model Parallelism},
    author = {Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick
              LeGresley and Jared Casper and Bryan Catanzaro},
    year = {2020},
    eprint = {1909.08053},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}
@article{bandwidthOptimalAllReduce2009,
    title = {Bandwidth optimal all-reduce algorithms for clusters of
             workstations},
    author = {Pich Patarasuk and Xin Yuan},
    year = {2009},
    journal = {Journal of Parallel and Distributed Computing},
}
@article{pope2022efficiently,
    title = {Efficiently Scaling Transformer Inference},
    author = {Reiner Pope and Sholto Douglas and Aakanksha Chowdhery and Jacob
              Devlin and James Bradbury and Anselm Levskaya and Jonathan Heek and
              Kefan Xiao and Shivani Agrawal and Jeff Dean},
    year = {2022},
    eprint = {2211.05102},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}
@article{he2022brrrrfromfirstprinciples,
    author = {Horace He},
    title = {Making Deep Learning Go Brrrr From First Principles},
    year = {2022},
    url = {https://horace.io/brrr_intro.html},
}
@article{kipply_inference_math,
    author = {Carol Chen},
    title = {Transformer Inference Arithmetic},
    year = {2022},
    url = {https://kipp.ly/blog/transformer-inference-arithmetic/},
}
@article{chowdhery2022palm,
    title = {PaLM: Scaling Language Modeling with Pathways},
    author = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten
              Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung
              Won Chung and Charles Sutton and Sebastian Gehrmann and Parker
              Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and
              Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and
              Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson
              and Reiner Pope and James Bradbury and Jacob Austin and Michael
              Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm
              Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski
              and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam
              Fedus and Denny Zhou and Daphne Ippolito and David Luan and
              Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan
              Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and
              Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie
              Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and
              Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi
              Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele
              Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and
              Jeff Dean and Slav Petrov and Noah Fiedel},
    year = {2022},
    eprint = {2204.02311},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}
@article{holtzman2020curious,
    title = {The Curious Case of Neural Text Degeneration},
    author = {Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin
              Choi},
    year = {2020},
    eprint = {1904.09751},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}
@article{kaplan2020scaling,
    title = {Scaling Laws for Neural Language Models},
    author = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown
              and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford
              and Jeffrey Wu and Dario Amodei},
    year = {2020},
    eprint = {2001.08361},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}
@article{mccandlish2018empirical,
    title = {An Empirical Model of Large-Batch Training},
    author = {Sam McCandlish and Jared Kaplan and Dario Amodei and OpenAI Dota
              Team},
    year = {2018},
    eprint = {1812.06162},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{hoffmann2022training,
    title = {Training Compute-Optimal Large Language Models},
    author = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena
              Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las
              Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark
              and Tom Hennigan and Eric Noland and Katie Millican and George van
              den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero
              and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol
              Vinyals and Laurent Sifre},
    year = {2022},
    eprint = {2203.15556},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}
@article{su2022roformer,
    title = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
    author = {Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo
              Wen and Yunfeng Liu},
    year = {2022},
    eprint = {2104.09864},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}
@article{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico LebrÃ³n and Sumit Sanghai},
  year={2023},
  eprint={2305.13245},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
@article{touvron2023llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{hendrycks2023gaussian,
      title={Gaussian Error Linear Units (GELUs)},
      author={Dan Hendrycks and Kevin Gimpel},
      year={2023},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
