@article{korthikanti2022reducing,
    title = {Reducing Activation Recomputation in Large Transformer Models},
    author = {Vijay Korthikanti and Jared Casper and Sangkug Lym and Lawrence
              McAfee and Michael Andersch and Mohammad Shoeybi and Bryan
              Catanzaro},
    year = {2022},
    eprint = {2205.05198},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{vaswani2017attention,
    title = {Attention Is All You Need},
    author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob
              Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and
              Illia Polosukhin},
    year = {2017},
    eprint = {1706.03762},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}

@article{gpt2radford2019language,
    title = {Language models are unsupervised multitask learners},
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and
              Amodei, Dario and Sutskever, Ilya and others},
    journal = {OpenAI blog},
    volume = {1},
    number = {8},
    pages = {9},
    year = {2019},
}

@article{gpt3brown2020language,
    title = {Language Models are Few-Shot Learners},
    author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah
              and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and
              Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini
              Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom
              Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler
              and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark
              Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin
              Chess and Jack Clark and Christopher Berner and Sam McCandlish and
              Alec Radford and Ilya Sutskever and Dario Amodei},
    year = {2020},
    eprint = {2005.14165},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}


@article{gpt4openai2023,
    title = {GPT-4 Technical Report},
    author = {OpenAI},
    year = {2023},
    eprint = {2303.08774},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}

@article{ALiBi,
    author = {Ofir Press and Noah A. Smith and Mike Lewis},
    title = {Train Short, Test Long: Attention with Linear Biases Enables Input
             Length Extrapolation},
    journal = {CoRR},
    volume = {abs/2108.12409},
    year = {2021},
    url = {https://arxiv.org/abs/2108.12409},
    eprinttype = {arXiv},
    eprint = {2108.12409},
    timestamp = {Thu, 02 Sep 2021 14:42:29 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-2108-12409.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}

@article{xiong2020layer,
    title = {On Layer Normalization in the Transformer Architecture},
    author = {Ruibin Xiong and Yunchang Yang and Di He and Kai Zheng and Shuxin
              Zheng and Chen Xing and Huishuai Zhang and Yanyan Lan and Liwei
              Wang and Tie-Yan Liu},
    year = {2020},
    eprint = {2002.04745},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{yang2022tensor,
    title = {Tensor Programs V: Tuning Large Neural Networks via Zero-Shot
             Hyperparameter Transfer},
    author = {Greg Yang and Edward J. Hu and Igor Babuschkin and Szymon Sidor
              and Xiaodong Liu and David Farhi and Nick Ryder and Jakub Pachocki
              and Weizhu Chen and Jianfeng Gao},
    year = {2022},
    eprint = {2203.03466},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{dao2022flashattention,
    title = {FlashAttention: Fast and Memory-Efficient Exact Attention with
             IO-Awareness},
    author = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and
              Christopher Ré},
    year = {2022},
    eprint = {2205.14135},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}
@article{dao2023flashattention2,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Tri Dao},
  year={2023},
  eprint={2307.08691},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
@article{micikevicius2018mixed,
title = {Mixed Precision Training},
author = {Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory
          Diamos and Erich Elsen and David Garcia and Boris Ginsburg and
          Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao
          Wu},
year = {2018},
eprint = {1710.03740},
archivePrefix = {arXiv},
primaryClass = {cs.AI},
}
@article{shazeer2019fast,
title = {Fast Transformer Decoding: One Write-Head is All You Need},
author = {Noam Shazeer},
year = {2019},
eprint = {1911.02150},
archivePrefix = {arXiv},
primaryClass = {cs.NE},
}

@article{shoeybi2020megatronlm,
    title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using
             Model Parallelism},
    author = {Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick
              LeGresley and Jared Casper and Bryan Catanzaro},
    year = {2020},
    eprint = {1909.08053},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}
@article{bandwidthOptimalAllReduce2009,
    title = {Bandwidth optimal all-reduce algorithms for clusters of
             workstations},
    author = {Pich Patarasuk and Xin Yuan},
    year = {2009},
    journal = {Journal of Parallel and Distributed Computing},
}
@article{pope2022efficiently,
    title = {Efficiently Scaling Transformer Inference},
    author = {Reiner Pope and Sholto Douglas and Aakanksha Chowdhery and Jacob
              Devlin and James Bradbury and Anselm Levskaya and Jonathan Heek and
              Kefan Xiao and Shivani Agrawal and Jeff Dean},
    year = {2022},
    eprint = {2211.05102},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}
@article{he2022brrrrfromfirstprinciples,
    author = {Horace He},
    title = {Making Deep Learning Go Brrrr From First Principles},
    year = {2022},
    url = {https://horace.io/brrr_intro.html},
}
@article{kipply_inference_math,
    author = {Carol Chen},
    title = {Transformer Inference Arithmetic},
    year = {2022},
    url = {https://kipp.ly/blog/transformer-inference-arithmetic/},
}
@article{chowdhery2022palm,
    title = {PaLM: Scaling Language Modeling with Pathways},
    author = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten
              Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung
              Won Chung and Charles Sutton and Sebastian Gehrmann and Parker
              Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and
              Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and
              Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson
              and Reiner Pope and James Bradbury and Jacob Austin and Michael
              Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm
              Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski
              and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam
              Fedus and Denny Zhou and Daphne Ippolito and David Luan and
              Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan
              Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and
              Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie
              Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and
              Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi
              Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele
              Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and
              Jeff Dean and Slav Petrov and Noah Fiedel},
    year = {2022},
    eprint = {2204.02311},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}
@article{holtzman2020curious,
    title = {The Curious Case of Neural Text Degeneration},
    author = {Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin
              Choi},
    year = {2020},
    eprint = {1904.09751},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}
@article{kaplan2020scaling,
    title = {Scaling Laws for Neural Language Models},
    author = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown
              and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford
              and Jeffrey Wu and Dario Amodei},
    year = {2020},
    eprint = {2001.08361},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}
@article{mccandlish2018empirical,
    title = {An Empirical Model of Large-Batch Training},
    author = {Sam McCandlish and Jared Kaplan and Dario Amodei and OpenAI Dota
              Team},
    year = {2018},
    eprint = {1812.06162},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{hoffmann2022training,
    title = {Training Compute-Optimal Large Language Models},
    author = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena
              Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las
              Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark
              and Tom Hennigan and Eric Noland and Katie Millican and George van
              den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero
              and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol
              Vinyals and Laurent Sifre},
    year = {2022},
    eprint = {2203.15556},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}

@article{su2022roformer,
    title = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
    author = {Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo
              Wen and Yunfeng Liu},
    year = {2022},
    eprint = {2104.09864},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}

@article{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebrón and Sumit Sanghai},
  year={2023},
  eprint={2305.13245},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@article{touvron2023llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{hendrycks2023gaussian,
      title={Gaussian Error Linear Units (GELUs)},
      author={Dan Hendrycks and Kevin Gimpel},
      year={2023},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{liu2023ringattentionblockwisetransformers,
  title={Ring Attention with Blockwise Transformers for Near-Infinite Context},
  author={Hao Liu and Matei Zaharia and Pieter Abbeel},
  year={2023},
  eprint={2310.01889},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2310.01889},
}

@article{brandon2023stripedattentionfasterring,
      title={Striped Attention: Faster Ring Attention for Causal Transformers},
      author={William Brandon and Aniruddha Nrusimha and Kevin Qian and Zachary Ankner and Tian Jin and Zhiye Song and Jonathan Ragan-Kelley},
      year={2023},
      eprint={2311.09431},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.09431},
}

@article{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision},
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020},
}
@article{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929},
}
@article{leviathan2023fastinferencetransformersspeculative,
      title={Fast Inference from Transformers via Speculative Decoding},
      author={Yaniv Leviathan and Matan Kalman and Yossi Matias},
      year={2023},
      eprint={2211.17192},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.17192},
}
@article{rafailov2024directpreferenceoptimizationlanguage,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2024},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.18290},
}
@article{schulman2017proximalpolicyoptimizationalgorithms,
      title={Proximal Policy Optimization Algorithms},
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347},
}
@article{shazeer2020gluvariantsimprovetransformer,
      title={GLU Variants Improve Transformer},
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.05202},
}

@article{fedus2022switchtransformersscalingtrillion,
      title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
      author={William Fedus and Barret Zoph and Noam Shazeer},
      year={2022},
      eprint={2101.03961},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.03961},
}
@article{zhou2022mixtureofexpertsexpertchoicerouting,
      title={Mixture-of-Experts with Expert Choice Routing},
      author={Yanqi Zhou and Tao Lei and Hanxiao Liu and Nan Du and Yanping Huang and Vincent Zhao and Andrew Dai and Zhifeng Chen and Quoc Le and James Laudon},
      year={2022},
      eprint={2202.09368},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.09368},
}
@article{muennighoff2024olmoeopenmixtureofexpertslanguage,
      title={OLMoE: Open Mixture-of-Experts Language Models}, 
      author={Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hannaneh Hajishirzi},
      year={2024},
      eprint={2409.02060},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.02060}, 
}
@article{shazeer2017outrageouslylargeneuralnetworks,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1701.06538}, 
}
