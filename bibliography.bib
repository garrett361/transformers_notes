@article{korthikanti2022reducing,
    title = {Reducing Activation Recomputation in Large Transformer Models},
    author = {Vijay Korthikanti and Jared Casper and Sangkug Lym and Lawrence
              McAfee and Michael Andersch and Mohammad Shoeybi and Bryan
              Catanzaro},
    year = {2022},
    eprint = {2205.05198},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{vaswani2017attention,
    title = {Attention Is All You Need},
    author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob
              Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and
              Illia Polosukhin},
    year = {2017},
    eprint = {1706.03762},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}

@article{gpt2radford2019language,
    title = {Language models are unsupervised multitask learners},
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and
              Amodei, Dario and Sutskever, Ilya and others},
    journal = {OpenAI blog},
    volume = {1},
    number = {8},
    pages = {9},
    year = {2019},
}

@article{gpt3brown2020language,
    title = {Language Models are Few-Shot Learners},
    author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah
              and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and
              Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini
              Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom
              Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler
              and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark
              Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin
              Chess and Jack Clark and Christopher Berner and Sam McCandlish and
              Alec Radford and Ilya Sutskever and Dario Amodei},
    year = {2020},
    eprint = {2005.14165},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}


@article{gpt4openai2023,
    title = {GPT-4 Technical Report},
    author = {OpenAI},
    year = {2023},
    eprint = {2303.08774},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}

@article{ALiBi,
    author = {Ofir Press and Noah A. Smith and Mike Lewis},
    title = {Train Short, Test Long: Attention with Linear Biases Enables Input
             Length Extrapolation},
    journal = {CoRR},
    volume = {abs/2108.12409},
    year = {2021},
    url = {https://arxiv.org/abs/2108.12409},
    eprinttype = {arXiv},
    eprint = {2108.12409},
    timestamp = {Thu, 02 Sep 2021 14:42:29 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-2108-12409.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}

@article{xiong2020layer,
    title = {On Layer Normalization in the Transformer Architecture},
    author = {Ruibin Xiong and Yunchang Yang and Di He and Kai Zheng and Shuxin
              Zheng and Chen Xing and Huishuai Zhang and Yanyan Lan and Liwei
              Wang and Tie-Yan Liu},
    year = {2020},
    eprint = {2002.04745},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{yang2022tensor,
    title = {Tensor Programs V: Tuning Large Neural Networks via Zero-Shot
             Hyperparameter Transfer},
    author = {Greg Yang and Edward J. Hu and Igor Babuschkin and Szymon Sidor
              and Xiaodong Liu and David Farhi and Nick Ryder and Jakub Pachocki
              and Weizhu Chen and Jianfeng Gao},
    year = {2022},
    eprint = {2203.03466},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{dao2022flashattention,
    title = {FlashAttention: Fast and Memory-Efficient Exact Attention with
             IO-Awareness},
    author = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and
              Christopher RÃ©},
    year = {2022},
    eprint = {2205.14135},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}
@article{micikevicius2018mixed,
    title = {Mixed Precision Training},
    author = {Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory
              Diamos and Erich Elsen and David Garcia and Boris Ginsburg and
              Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao
              Wu},
    year = {2018},
    eprint = {1710.03740},
    archivePrefix = {arXiv},
    primaryClass = {cs.AI},
}
