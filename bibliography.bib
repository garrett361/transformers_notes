@article{korthikanti2022reducing,
    title = {Reducing Activation Recomputation in Large Transformer Models},
    author = {Vijay Korthikanti and Jared Casper and Sangkug Lym and Lawrence
              McAfee and Michael Andersch and Mohammad Shoeybi and Bryan
              Catanzaro},
    year = {2022},
    eprint = {2205.05198},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{vaswani2017attention,
    title = {Attention Is All You Need},
    author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob
              Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and
              Illia Polosukhin},
    year = {2017},
    eprint = {1706.03762},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}

@article{gpt2radford2019language,
    title = {Language models are unsupervised multitask learners},
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and
              Amodei, Dario and Sutskever, Ilya and others},
    journal = {OpenAI blog},
    volume = {1},
    number = {8},
    pages = {9},
    year = {2019},
}

@article{gpt3brown2020language,
    title = {Language Models are Few-Shot Learners},
    author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah
              and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and
              Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini
              Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom
              Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler
              and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark
              Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin
              Chess and Jack Clark and Christopher Berner and Sam McCandlish and
              Alec Radford and Ilya Sutskever and Dario Amodei},
    year = {2020},
    eprint = {2005.14165},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}


@article{gpt4openai2023,
    title = {GPT-4 Technical Report},
    author = {OpenAI},
    year = {2023},
    eprint = {2303.08774},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}

@article{ALiBi,
    author = {Ofir Press and Noah A. Smith and Mike Lewis},
    title = {Train Short, Test Long: Attention with Linear Biases Enables Input
             Length Extrapolation},
    journal = {CoRR},
    volume = {abs/2108.12409},
    year = {2021},
    url = {https://arxiv.org/abs/2108.12409},
    eprinttype = {arXiv},
    eprint = {2108.12409},
    timestamp = {Thu, 02 Sep 2021 14:42:29 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-2108-12409.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}

@article{xiong2020layer,
    title = {On Layer Normalization in the Transformer Architecture},
    author = {Ruibin Xiong and Yunchang Yang and Di He and Kai Zheng and Shuxin
              Zheng and Chen Xing and Huishuai Zhang and Yanyan Lan and Liwei
              Wang and Tie-Yan Liu},
    year = {2020},
    eprint = {2002.04745},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{yang2022tensor,
    title = {Tensor Programs V: Tuning Large Neural Networks via Zero-Shot
             Hyperparameter Transfer},
    author = {Greg Yang and Edward J. Hu and Igor Babuschkin and Szymon Sidor
              and Xiaodong Liu and David Farhi and Nick Ryder and Jakub Pachocki
              and Weizhu Chen and Jianfeng Gao},
    year = {2022},
    eprint = {2203.03466},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{dao2022flashattention,
    title = {FlashAttention: Fast and Memory-Efficient Exact Attention with
             IO-Awareness},
    author = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and
              Christopher RÃ©},
    year = {2022},
    eprint = {2205.14135},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}
@article{dao2023flashattention2,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Tri Dao},
  year={2023},
  eprint={2307.08691},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
@article{micikevicius2018mixed,
title = {Mixed Precision Training},
author = {Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory
          Diamos and Erich Elsen and David Garcia and Boris Ginsburg and
          Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao
          Wu},
year = {2018},
eprint = {1710.03740},
archivePrefix = {arXiv},
primaryClass = {cs.AI},
}

@article{shazeer2019fast,
title = {Fast Transformer Decoding: One Write-Head is All You Need},
author = {Noam Shazeer},
year = {2019},
eprint = {1911.02150},
archivePrefix = {arXiv},
primaryClass = {cs.NE},
}

@article{shoeybi2020megatronlm,
    title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using
             Model Parallelism},
    author = {Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick
              LeGresley and Jared Casper and Bryan Catanzaro},
    year = {2020},
    eprint = {1909.08053},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}
@article{bandwidthOptimalAllReduce2009,
    title = {Bandwidth optimal all-reduce algorithms for clusters of
             workstations},
    author = {Pich Patarasuk and Xin Yuan},
    year = {2009},
    journal = {Journal of Parallel and Distributed Computing},
}
@article{pope2022efficiently,
    title = {Efficiently Scaling Transformer Inference},
    author = {Reiner Pope and Sholto Douglas and Aakanksha Chowdhery and Jacob
              Devlin and James Bradbury and Anselm Levskaya and Jonathan Heek and
              Kefan Xiao and Shivani Agrawal and Jeff Dean},
    year = {2022},
    eprint = {2211.05102},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}
@article{he2022brrrrfromfirstprinciples,
    author = {Horace He},
    title = {Making Deep Learning Go Brrrr From First Principles},
    year = {2022},
    url = {https://horace.io/brrr_intro.html},
}
@article{kipply_inference_math,
    author = {Carol Chen},
    title = {Transformer Inference Arithmetic},
    year = {2022},
    url = {https://kipp.ly/blog/transformer-inference-arithmetic/},
}
@article{chowdhery2022palm,
    title = {PaLM: Scaling Language Modeling with Pathways},
    author = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten
              Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung
              Won Chung and Charles Sutton and Sebastian Gehrmann and Parker
              Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and
              Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and
              Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson
              and Reiner Pope and James Bradbury and Jacob Austin and Michael
              Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm
              Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski
              and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam
              Fedus and Denny Zhou and Daphne Ippolito and David Luan and
              Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan
              Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and
              Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie
              Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and
              Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi
              Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele
              Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and
              Jeff Dean and Slav Petrov and Noah Fiedel},
    year = {2022},
    eprint = {2204.02311},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}
@article{holtzman2020curious,
    title = {The Curious Case of Neural Text Degeneration},
    author = {Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin
              Choi},
    year = {2020},
    eprint = {1904.09751},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}
@article{kaplan2020scaling,
    title = {Scaling Laws for Neural Language Models},
    author = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown
              and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford
              and Jeffrey Wu and Dario Amodei},
    year = {2020},
    eprint = {2001.08361},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}
@article{mccandlish2018empirical,
    title = {An Empirical Model of Large-Batch Training},
    author = {Sam McCandlish and Jared Kaplan and Dario Amodei and OpenAI Dota
              Team},
    year = {2018},
    eprint = {1812.06162},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
}

@article{hoffmann2022training,
    title = {Training Compute-Optimal Large Language Models},
    author = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena
              Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las
              Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark
              and Tom Hennigan and Eric Noland and Katie Millican and George van
              den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero
              and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol
              Vinyals and Laurent Sifre},
    year = {2022},
    eprint = {2203.15556},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}

@article{su2022roformer,
    title = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
    author = {Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo
              Wen and Yunfeng Liu},
    year = {2022},
    eprint = {2104.09864},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
}

@article{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico LebrÃ³n and Sumit Sanghai},
  year={2023},
  eprint={2305.13245},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@article{touvron2023llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{hendrycks2023gaussian,
      title={Gaussian Error Linear Units (GELUs)},
      author={Dan Hendrycks and Kevin Gimpel},
      year={2023},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{liu2023ringattentionblockwisetransformers,
  title={Ring Attention with Blockwise Transformers for Near-Infinite Context},
  author={Hao Liu and Matei Zaharia and Pieter Abbeel},
  year={2023},
  eprint={2310.01889},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2310.01889},
}

@article{brandon2023stripedattentionfasterring,
      title={Striped Attention: Faster Ring Attention for Causal Transformers},
      author={William Brandon and Aniruddha Nrusimha and Kevin Qian and Zachary Ankner and Tian Jin and Zhiye Song and Jonathan Ragan-Kelley},
      year={2023},
      eprint={2311.09431},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.09431},
}

@article{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision},
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020},
}
@article{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929},
}
@article{leviathan2023fastinferencetransformersspeculative,
      title={Fast Inference from Transformers via Speculative Decoding},
      author={Yaniv Leviathan and Matan Kalman and Yossi Matias},
      year={2023},
      eprint={2211.17192},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.17192},
}
@article{rafailov2024directpreferenceoptimizationlanguage,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2024},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.18290},
}
@article{schulman2017proximalpolicyoptimizationalgorithms,
      title={Proximal Policy Optimization Algorithms},
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347},
}
@article{shazeer2020gluvariantsimprovetransformer,
      title={GLU Variants Improve Transformer},
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.05202},
}

@article{fedus2022switchtransformersscalingtrillion,
      title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
      author={William Fedus and Barret Zoph and Noam Shazeer},
      year={2022},
      eprint={2101.03961},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.03961},
}
@article{zhou2022mixtureofexpertsexpertchoicerouting,
      title={Mixture-of-Experts with Expert Choice Routing},
      author={Yanqi Zhou and Tao Lei and Hanxiao Liu and Nan Du and Yanping Huang and Vincent Zhao and Andrew Dai and Zhifeng Chen and Quoc Le and James Laudon},
      year={2022},
      eprint={2202.09368},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.09368},
}
@article{muennighoff2024olmoeopenmixtureofexpertslanguage,
      title={OLMoE: Open Mixture-of-Experts Language Models},
      author={Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hannaneh Hajishirzi},
      year={2024},
      eprint={2409.02060},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.02060},
}
@article{shazeer2017outrageouslylargeneuralnetworks,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1701.06538},
}
@article{gale2022megablocksefficientsparsetraining,
      title={MegaBlocks: Efficient Sparse Training with Mixture-of-Experts},
      author={Trevor Gale and Deepak Narayanan and Cliff Young and Matei Zaharia},
      year={2022},
      eprint={2211.15841},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.15841},
}
@article{ethayarajh2024ktomodelalignmentprospect,
      title={KTO: Model Alignment as Prospect Theoretic Optimization},
      author={Kawin Ethayarajh and Winnie Xu and Niklas Muennighoff and Dan Jurafsky and Douwe Kiela},
      year={2024},
      eprint={2402.01306},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.01306},
}

@article{s4,
  author       = {Albert Gu and
                  Karan Goel and
                  Christopher R{\'{e}}},
  title        = {Efficiently Modeling Long Sequences with Structured State Spaces},
  journal      = {CoRR},
  volume       = {abs/2111.00396},
  year         = {2021},
  url          = {https://arxiv.org/abs/2111.00396},
  eprinttype    = {arXiv},
  eprint       = {2111.00396},
  timestamp    = {Fri, 05 Nov 2021 15:25:54 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2111-00396.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{mamba,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
      author={Albert Gu and Tri Dao},
      year={2024},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00752},
}
@article{physicalDL,
  author       = {Daniel A. Roberts and
                  Sho Yaida and
                  Boris Hanin},
  title        = {The Principles of Deep Learning Theory},
  journal      = {CoRR},
  volume       = {abs/2106.10165},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.10165},
  eprinttype    = {arXiv},
  eprint       = {2106.10165},
  timestamp    = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-10165.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{doshi2023criticalinitializationwidedeep,
      title={Critical Initialization of Wide and Deep Neural Networks through Partial Jacobians: General Theory and Applications},
      author={Darshil Doshi and Tianyu He and Andrey Gromov},
      year={2023},
      eprint={2111.12143},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.12143},
}
@article{yaida2022metaprincipledfamilyhyperparameterscaling,
      title={Meta-Principled Family of Hyperparameter Scaling Strategies},
      author={Sho Yaida},
      year={2022},
      eprint={2210.04909},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.04909},
}
@article{prefixSumsBlelloch,
      title={Prefix Sums and their Applications},
      author={Guy E. Blelloch},
      url={https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf},
}
@article{katharopoulos2020transformersrnnsfastautoregressive,
      title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
      author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and FranÃ§ois Fleuret},
      year={2020},
      eprint={2006.16236},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.16236},
}
@article{dao2024transformersssmsgeneralizedmodels,
      title={Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
      author={Tri Dao and Albert Gu},
      year={2024},
      eprint={2405.21060},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.21060},
}
@article{chen2023symbolicdiscoveryoptimizationalgorithms,
      title={Symbolic Discovery of Optimization Algorithms},
      author={Xiangning Chen and Chen Liang and Da Huang and Esteban Real and Kaiyuan Wang and Yao Liu and Hieu Pham and Xuanyi Dong and Thang Luong and Cho-Jui Hsieh and Yifeng Lu and Quoc V. Le},
      year={2023},
      eprint={2302.06675},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.06675},
}

@article{chiuPscanDiff,
      title={Differentiating through an associative parallel scan},
      author={Justin T. Chiu},
      year={2024},
      url={https://justintchiu.com/blog/pscan_diff/},
}

@article{deepseekai2024deepseekv2strongeconomicalefficient,
      title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bin Wang and Bingxuan Wang and Bo Liu and Chenggang Zhao and Chengqi Dengr and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Hanwei Xu and Hao Yang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jin Chen and Jingyang Yuan and Junjie Qiu and Junxiao Song and Kai Dong and Kaige Gao and Kang Guan and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruizhe Pan and Runxin Xu and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Size Zheng and T. Wang and Tian Pei and Tian Yuan and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Liu and Xin Xie and Xingkai Yu and Xinnan Song and Xinyi Zhou and Xinyu Yang and Xuan Lu and Xuecheng Su and Y. Wu and Y. K. Li and Y. X. Wei and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Zheng and Yichao Zhang and Yiliang Xiong and Yilong Zhao and Ying He and Ying Tang and Yishi Piao and Yixin Dong and Yixuan Tan and Yiyuan Liu and Yongji Wang and Yongqiang Guo and Yuchen Zhu and Yuduan Wang and Yuheng Zou and Yukun Zha and Yunxian Ma and Yuting Yan and Yuxiang You and Yuxuan Liu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhewen Hao and Zhihong Shao and Zhiniu Wen and Zhipeng Xu and Zhongyu Zhang and Zhuoshu Li and Zihan Wang and Zihui Gu and Zilin Li and Ziwei Xie},
      year={2024},
      eprint={2405.04434},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.04434},
}

@article{deepseekai2025deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report},
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},
      year={2025},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437},
}
@article{ultrascale_playbook,
      title={The Ultra-Scale Playbook: Training LLMs on GPU Clusters},
      author={Nouamane Tazi, Ferdinand Mom, Haojun Zhao, Phuc Nguyen, Mohamed Mekkouri, Leandro Werra, Thomas Wolf},
      year={2025},
}
@article{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models},
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco GuzmÃ¡n and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Ãelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and VÃ­tor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783},
}
@article{huang2019gpipeefficienttraininggiant,
      title={GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
      author={Yanping Huang and Youlong Cheng and Ankur Bapna and Orhan Firat and Mia Xu Chen and Dehao Chen and HyoukJoong Lee and Jiquan Ngiam and Quoc V. Le and Yonghui Wu and Zhifeng Chen},
      year={2019},
      eprint={1811.06965},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1811.06965},
}
@article{harlap2018pipedreamfastefficientpipeline,
      title={PipeDream: Fast and Efficient Pipeline Parallel DNN Training},
      author={Aaron Harlap and Deepak Narayanan and Amar Phanishayee and Vivek Seshadri and Nikhil Devanur and Greg Ganger and Phil Gibbons},
      year={2018},
      eprint={1806.03377},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/1806.03377},
}
@article{narayanan2021efficientlargescalelanguagemodel,
      title={Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM},
      author={Deepak Narayanan and Mohammad Shoeybi and Jared Casper and Patrick LeGresley and Mostofa Patwary and Vijay Anand Korthikanti and Dmitri Vainbrand and Prethvi Kashinkunti and Julie Bernauer and Bryan Catanzaro and Amar Phanishayee and Matei Zaharia},
      year={2021},
      eprint={2104.04473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.04473},
}
@misc{lamypoirier2023breadthfirstpipelineparallelism,
      title={Breadth-First Pipeline Parallelism},
      author={Joel Lamy-Poirier},
      year={2023},
      eprint={2211.05953},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2211.05953},
}
@article{qi2023zerobubblepipelineparallelism,
      title={Zero Bubble Pipeline Parallelism}, 
      author={Penghui Qi and Xinyi Wan and Guangxing Huang and Min Lin},
      year={2023},
      eprint={2401.10241},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2401.10241}, 
}
