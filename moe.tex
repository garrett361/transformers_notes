\part{Mixture of Experts}

\section{Basics}

The $ \Ocal \left(  D ^{ 2 } \right)  $ FLOPs count due to MLP layers\footnote{The $ \Ocal \left( S
^{ 2 } \right)  $ scaling of the self-attention layers is also untenable, but MoE only addresses the
MLP layers.} is untenable past a given point: inference and training just take too long.  Mixture of
Experts\footnote{The original MoE research came out of Google: see
    \cite{fedus2022switchtransformersscalingtrillion},
    \cite{shazeer2017outrageouslylargeneuralnetworks} and related work by these authors. An
excellent MoE paper with open-source everything is here
\cite{muennighoff2024olmoeopenmixtureofexpertslanguage}. } (MoE) models address this concern by
splitting single MLP layer into a number ``expert" MLP layers and route a subset of the tokens to a
subset of the experts."  Comparing a dense and a MoE model at fixed parameter count, the expert
layer's intermediate dimension is reduced by $ \Ocal \left( N _{ {\rm ex} } \right)  $, the number
of experts, and the FLOPs count is also reduced by this factor. MoE is a lever for changing the
relation between the per-token FLOPs count and the overall parameter count. Perhaps unsurprisingly,
MoE experts outperform and train faster than their FLOPs equivalent dense models (at the cost of
more engineering complexity and a higher memory burden).

The general form of the MoE layer output is
\begin{align}
    z' _{ sd } &=G _{ se }(z _{ sd }, \ldots )E _{ e } \left ( z _{ sd } \right )
\end{align}k
where $ G _{ se }(z _{ sd }) \in \mathbb{R} ^{ S \times N _{ {\rm ex}  } } $ is a gating (i.e.,
weighting) function and $ E _{ e } \left ( z _{ sd } \right ) $ is the usual MLP operation performed
by the $ e $-th expert. Many of the entries $ G _{ es } $  are zero in practice, and only the
computations $ E _{ e } \left ( z _{ sd } \right ) $ corresponding to non-trivial gating values are
performed, of course. Different MoE variants are essentially differentiated by the specific form of
their weighting function.


\section{Routing}


Choosing which experts process which tokens is crucial, affecting both the downstream model and
engineering (i.e. throughput) performance.  There are two dominant schemes:
\begin{enumerate}
    \item \textbf{Token Choice}: each token selects a fixed number of experts.
    \item \textbf{Expert Choice}: each expert selects a fixed number of tokens.
\end{enumerate}
Layered on top of this choice are the details of the routing mechanisms.

\subsection{Token Choice vs Expert Choice}

Token and expert choice both introduce a tensor $W _{ de } \in \mathbb{R} ^{ D\times N _{ {\rm  ex}
} }$ which is used to produce a score between each token and expert: $ S _{ se } = z _{ sd } W _{ de
} $. In each case, we perform a \texttt{topk} computation and output a weighted sum of expert
outputs: the two methods just differ in the dimension over which the \texttt{topk} is performed.

For expert choice, the gating function is
\begin{align}
    G  ^{ {\rm  expert\ choice} }_{ se }(z _{ sd }, W)  &= \texttt{Softmax}_{ e } \left ( \texttt{topk} _{ e } \left ( z _{ sd } \cdot W _{ de }  \right ) \right ) \ , \label{eq_expert_choice}
\end{align}
where we're taking the \texttt{topk} to set all non-top-$ k $ entries to $ -\infty $. So, $ G _{ se
} $ is a matrix with $ Sk $ non-trivial elements. A (potential) disadvantage of expert choice is
that some tokens will not be routed to any expert at all, but every expert has an equal load.

Token choice just performs the \texttt{Softmax} and \texttt{topk} on the sequence dimension, instead
\begin{align}
    G  ^{ {\rm  token\ choice} }_{ se }(z _{ sd }, W)  &= \texttt{Softmax}_{ s } \left ( \texttt{topk} _{ s } \left ( z _{ sd } \cdot W _{ de }  \right ) \right ) \ , \label{eq_token_choice}
\end{align}
where we we again set all non-top-$ k $ scores to $ -\infty $ before taking the \texttt{Softmax}.
$ G _{ se } $ has $ kE $ non-trivial elements in this case. While every token will get routed to $ k
$ experts with token choice routing, the per-expert load can be very unbalanced.



% \subsection{Expert Choice}
% By design, expert choice routing \cite{zhou2022mixtureofexpertsexpertchoicerouting} perfectly
% distributes compute load across experts, which is engineering performance optimal, but not
% necessarily model-optimal (as found by \cite{muennighoff2024olmoeopenmixtureofexpertslanguage}).
% Another disadvantage of expert choice is that it may drop tokens: some tokens may not be selected by
% \textit{any} expert.
%
% Expert choice introduces a tensor $ W _{ de } \in \mathbb{R} ^{ D\times N _{ {\rm  ex} } } $ and
% routes a given token to $ k $ experts\footnote{$ k $ relates to the \textit{capacity factor} $ c $
% via $ k \equiv c S / N _{ {\rm ex} }$.} chosen by taking the top-$ k$ entries of $ w _{ se }=  z _{
% sd }\cdot W _{ de } $ along the expert ($ e $) dimension for each sequence position. The outputs of
% the corresponding experts are then combined with relative weights corresponding to
% \pyinline{Softmax}-ing the expert dimension\footnote{The original paper seems to first
% \pyinline{Softmax} and \textit{then} extract the top-$ k $, which would add up expert outputs with
% weights that do not sum to one, but that is probably not standard practice.} of $ w _{ se } $.
%





\section{Shared Experts}


Shared experts forces one particular expert to always be used.
