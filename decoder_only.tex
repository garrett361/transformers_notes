\documentclass[11pt]{article}
%Importing custom commands
\usepackage{latex_goon/latex_goon}
\title{Decoder-Only Transformers}
\author{Garrett Goon}
\begin{document}
%
%\maketitle

\vspace{1truecm}
%
%
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\begin{center}
{\huge \bf{Decoder-Only Transformers}}
\end{center}


\begin{abstract}

Notes on various aspects of Decoder-Only Transformers.

\end{abstract}

\tableofcontents


\renewcommand*{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{0}

\part{Architecture}

\section{Decoder-Only Fundamentals \label{sec_decoder_only} }

The Transformers architecture \cite {vaswani2017attention}, which dominates Natural Language
Processing (NLP) as of July 2023, is a relatively simple architecture. There are various flavors and
variants of Tranformers, but focus here on the decoder-only versions which underlie the
GPT models \cite {gpt2radford2019language, gpt3brown2020language, gpt4openai2023}.

The full decoder-only architecture can be seen in Fig.~\ref{fig_transformers_architecture}. The
parameters which define the network can be found in App.~\ref{app_conventions}.
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.28]{figures/transformer-general.jpg}
    \caption{The full transformers architecture. Diagram taken from \cite{korthikanti2022reducing} }
    \label{fig_transformers_architecture}
\end{figure}


At a high level, decoder-only transformers take in a series of word-like objects, called tokens, and are trained
to predict the next token in the sequence. An outline of the mechanics:
\begin{enumerate}
    \item Raw text is \textbf{tokenized} and turned into a series of integers\footnote{There are
        about \href{https://github.com/ray-project/llm-numbers}{1.3 tokens per word}, on average.} whose values lie in \pyinline{range(V)}, with $ V $ the vocabulary
        size.
    \item The tokenized text is chunked and turned into \pyinline{(B, S)}-shaped (batch size and
        sequence length, respectively) integer tensors, $ x _{ bs } $.
    \item The \textbf{embedding layer} converts the integer tensors into continuous representations of shape
        \pyinline{(B, S, D)}, $ z _{ bsd } $, with $ D $ the size of the hidden dimension.
        \textbf{Positional encodings} have also been added to the tensor at this stage to help the
        architecture understand the relative ordering of the text.
    \item The $ z _{ bsd } $ tensors pass through a series of transformer blocks, each of which has
        two primary components:
        \begin{enumerate}
        \item In the \textbf{attention} sub-block, components of $ z _{ bsd } $ at different
            positions ($ s $-values) interact with each other, resulting in another \pyinline{(B, S, D)}-shaped
            tensor, $  z' _{ bsd } $.
        \item In the \textbf{MLP} block, each position in  $ z' _{ bsd } $ is processed
            independently and in parallel by a two-layer feed-forward network, resulting once more
            in a \pyinline{(B, S, D)}-shaped tensor.
        \end{enumerate}
        Importantly, there are \textbf{residual connections} around each of these\footnote{This
        gives rise to the concept of the \textbf{residual stream} which each transformer block reads
        from and writes back to repeatedly.} (the arrows in Fig.~\ref{fig_transformers_architecture}).
    \item Finally, we convert the \pyinline{(B, S, D)}-shaped
        tensors to \pyinline{(B, S, V)}-shaped ones, $ y _{ bsv } $. This is the role of
        the \textbf{language model head} (which is often just the embedding layer used in an inverse
        manner.)
    \item  The $ y _{ bsv } $ predict what the next token will be, i.e. $ x _{ bs+1 } $, having seen the \textbf{context}
        of the first $ s $ tokens in the sequence.
\end{enumerate}


Each batch (the $ b $-index) is processed independently. We omitted \pyinline{LayerNorm} and
\pyinline{Dropout} layers above, as well as the causal mask; these will be covered below as we step
through the architecture in more detail.


We break down the various components below in detail.

\subsection{Embedding Layer and Positional Encodings \label{subsubsec_embedding_and_pe} }

The \textbf{embedding} layer is just a simple look up table: each of the \pyinline{range(V)} indices
in the vocabulary is mapped to a $ D $-dimensional vector via a large \pyinline{(V, D)}-shaped
table/matrix. This layer maps $ x _{ bs } \longrightarrow z _{ bsd } $. In \pyinline{torch}, this is
an \pyinline{nn.Embedding(V, D)} instance.

To each item in a batch, we add identical \textbf{positional encodings} to the vectors above with
the goal of adding fixed, position-dependent correlations in the sequence dimension which will
hopefully make it easier for the architecture to pick up on the relative positions of the inputs
\footnote{Positional encodings and the causal mask are the only components in the transformers
architecture which carry weights with a dimension of size $ S $; i.e. they are the only parts that
have explicit sequence-length dependence. A related though experiment: you can convince yourself
that if the inputs $ z_{ bsd } $
were just random noise, the transformers architecture would not be able to predict
the $ s $-index of each such input in the absence of positional encodings. } This layer maps $ z _{
bsd} \leftarrow z _{ bsd } + p _{ sd } $, with $ p _{ sd } $ the positional encoding tensor.

The above components require $ (V+S)D \approx VD $ parameters per layer.



\subsection{Layer Norm \label{subsubsec_layer_norm} }

The original transformers paper \cite{vaswani2017attention} put \pyinline{LayerNorm} instances after
the \textbf{attention} and \textbf{MLP} blocks, but now it is common \cite{xiong2020layer} to put
them before these blocks\footnote{Which makes intuitive sense for the purposes of stabilizing the
matrix multiplications in the blocks}.

The \pyinline{LayerNorm} operations acts over the sequence dimension. Spelling it out, given the
input tensor $ z _{ bsd } $ whose mean and variance over the $ s $-index are $ \mu _{ bd } $ and $
\sigma _{ bd } $, respectively, the \pyinline{LayerNorm} output is
\begin{align}
  z _{ bsd } &\leftarrow \left ( \frac{ z _{ bsd } - \mu _{ bd } }{ \sigma _{ bd } } \right )\times \gamma _{ d }
  + \beta _{ d } \equiv \LN _{ s } z _{ bsd}
\end{align}
where $ \gamma _{ d }, \beta  _{ d } $ are the trainable scale and bias parameters. In
\pyinline{torch}, this is a \pyinline{nn.LayerNorm(D)} instance.

Since there are two \pyinline{LayerNorm} instances in each transformer block, these components require
$ 2D $ parameters per layer.


\subsection{Causal Attention \label{subsubsec_attn_layer} }

The \textbf{causal attention} layer is the most complex layer. It features $ H $  triplets\footnote{$ H $
must divide the hidden dimension $ D $ evenly.} of weight matrices\footnote{There are also bias
terms, but we will often neglect to write them explicitly or account for their (negligible)
parameter count.}  $ Q ^{ h } _{ d f }, K ^{ h } _{ df }, V ^{ h } _{ df }  $
where $ a \in \left \{ 0, \ldots, H-1 \right \} $ and $ f \in \left \{ 0, \ldots, D/H \right \} $.
From these, we form three different vectors:
\begin{align}
  q ^{ h }_{ bsf } &= z _{ bsd } Q ^{ h }_{ df } \ , \quad
  k ^{ h }_{ bsf } = z _{ bsd } K ^{ h }_{ df }  \ , \quad
  v ^{ h }_{ bsf } = z _{ bsd } V ^{ h }_{ df }
\end{align}
These are the \textbf{query, key, and value} tensors, respectively \footnote{There are of course
many variants of the architecture and one variant which is popular in Summer 2023 is multi-query
attention \cite{shazeer2019fast} in which all heads share \textit{the same} key and value vectors
and only the query changes across heads, as this greatly reduces inference costs.}.

Using the above tensors, we will then build up an \textbf{attention map}  $ w ^{ h }_{ bss' } $
which corresponds to how much attention the token at position $ s $ pays to the token at
position $ s' $.  Because we have the goal of predicting the
next token in the sequence, we need these weights to be causal: the final prediction $ y _{ bsv } $
should only have access to information propagated from positions $ x _{ bs'v } $ with $ s' \le s $.
This corresponds to the condition that $ w ^{ h }_{ bss' } = 0  $ if  $ s' > s  $.

 These weights come from \pyinline{Softmax}-ed attention scores, which are just a normalized
 dot-product over the hidden dimension:
\begin{align}
    w ^{ h }_{ bss'd } &=\Sm _{ s' } \left (m _{ s s' }+\ q ^{ h }_{ bsf }k ^{ h }_{ bs'f } / \sqrt{D/H} \right
        )\ ,  \quad {\rm s.t.} \quad \sum_{s'}w ^{ h }_{ bdss' } =1
\end{align}
The tensor $ m _{  s s' } $ is the causal mask which zeroes out the relevant attention map
components above
\begin{align}
    m _{ s s' } &= \begin{cases}
            0 & s \le s' \nn
            -\infty &= s > s'
        \end{cases}\ .
\end{align}
In other words, the causal mask ensures that a given tensor, say $ z _{ bsd } $, only has dependence on
other tensors whose sequence index, say $ s' $, with $ s' \le s $.

The $ \sqrt{D/H} $ normalization is motivated by demanding
that the variance of the \pyinline{Softmax} argument be 1 at initialization, assuming that other
components have been configured so that that the query and key components are i.i.d. from a Gaussian
normal distribution \footnote{However, in \cite{yang2022tensor} it is instead argued that no square
root should be taken in order to maximize the speed of learning via SGD.}.

The weights above are then passed through a dropout layer and used to re-weigh the \textbf{value} vectors and form the tensors
\begin{align}
  t ^{ h }_{ bsf }&=\Dr  \left (w ^{ h }_{ bdss' } \right ) v ^{ h }_{ bs'f }
  \label{eq_reweighted_values}
\end{align}
and these $ H $ different \pyinline{(B, S, D/H)}-shaped tensors
are then concatenated along the $ f $-direction to re-form a \pyinline{(B, S, D)}-shaped
tensor\footnote{It is hard to come up with good index-notation for concatenation.}
\begin{align}
    u _{ bsd } &= {\rm Concat}^{ a }_{ fd } \left ( t ^{ a }_{ bsf }\right )\ .
\end{align}
Finally, another weight matrix $ O _{d' d } $ and dropout layer transform the output once again to get the final
output
\begin{align}
  z _{ bsd } &= \Dr \left (u  _{ bsd' } O _{ d'd }\right )\ .
\end{align}

For completeness, the entire operation in condensed notation with indices left implicit is:
\begin{align}
    z  &\leftarrow \Dr \left ({\rm Concat} \ \left ( \Dr \left (\Sm  \left ( \frac{ \left ( z \cdot Q ^{ h } \right )\cdot \left ( z \cdot K ^{ h } \right )}{ \sqrt{D/H} }
            \right)\right )\cdot z \cdot V ^{ h } \right ) \cdot O \right ) \label{eq_causal_attn}
\end{align}
where all of the dot-products are over feature dimensions (those of size $ D $ or $ D/H $).

The final \pyinline{Dropout} layer is often included as part of the \pyinline{CausalAttention} block , as
in the math above and in Fig.~\ref{fig_transformers_architecture}. Below is pedagogical\footnote{The
code is written for clarity, not speed. An example optimization missing here: there is no need to
form separate $ Q ^{ h },K ^{ h },V ^{ h} $ \pyinline{Linear} layers, one large layer which is later
chunked is more efficient} sample code for a \pyinline{CausalAttention} layer which separates the
last \pyinline{Dropout} layer out. This will be convenient when we talk about sequence parallelism below
in Sec.~\ref{subsec_seq_parallelism}. \pyfile[firstline=10,lastline=57]{python/causal_attention.py}

The parameter count is dominated by the weight matrices which carry $ 4 D ^{ 2 } $ total parameters per layer.


\subsection{MLP \label{subsubsec_mlp} }

The feed-forward network is straightforward and corresponds to
\begin{align}
  z _{ bsd } &\leftarrow \Dr \left (\phi \left ( z _{ bsd' }W ^{ 0 }_{ d'e } \right ) W ^{ 1 } _{ ed
      } \right ) \label{eq_mlp}
\end{align}
where $ W ^{ 0 } $ and $ W ^{ 1 } $ are \pyinline{(D, ED)}- and \pyinline{(ED, D)}-shaped matrices,
respectively (see App.~\ref{app_conventions} for notation) and $ \phi $ is a
non-linearity\footnote{The \pyinline{GeLU}
\href{https://pytorch.org/docs/stable/generated/torch.nn.GELU.html}{non-linearity} is common.}.
In code, where we again separate out the last \pyinline{Dropout} layer as we did in in
Sec.~\ref{subsubsec_attn_layer}.  \pyfile[firstline=8, lastline=23]{python/mlp.py}

This bock requires $ 2 E D ^{ 2 } $ parameters per layer, only counting the contribution from
weights.


\subsection{Language Model Head \label{subsubsec_language_model_head} }


The layer which converts the \pyinline{(B, S, D)}-shaped outputs, $ z _{ bsd } $, to \pyinline{(B, S, V)}-shaped
predictions over the vocabulary, $  y _{ bsv } $, is the \textbf{Language Model Head}. It
is a linear layer, whose weights are usually tied to be exactly those of the initial embedding
layer of Sec.~\ref{subsubsec_embedding_and_pe}.


\subsection{All Together}
It is then relatively straightforward to tie everything together.  In code, we can first create a
transformer block like
\pyfile[firstline=10, lastline=32]{python/transformer_block.py}
which corresponds to the schematic function
\begin{align}
  z &\leftarrow  z + \texttt{MLP}\left ( \LN \left ( z + \texttt{CausalAttention}\left ( \LN \left (
  z\right ) \right )  \right ) \right )\ ,
\end{align}
indices suppressed.

And then the entire architecture: \pyfile[firstline=9, lastline=50]{python/decoder_only.py}


\subsection{The Loss Function}

The last necessary component is the loss function. The training loop data is canonically the
\pyinline{(B, K)}-shaped\footnote{\pyinline{K} is the block size, the maximum sequence-length for
the model. See App.~\ref{app_conventions}.}  token inputs ($ x _{ bs } $) along with their shifted-by-one relatives $ y
_{ bs }$ where \pyinline{x[:, s + 1] == y[:, s]}.  The \pyinline{(B, K, V)}-shaped
outputs ($ z _{ bsv } $)  of the \pyinline{DecoderOnly} network are treated as the logits which
predict the value of the next token, given the present context:
\begin{align}
    p(x _{ b (s+1) }=v| x _{ b s }, x _{ b (s-1) }, \ldots, x _{ b 0 }) &= \Sm _{ v }\ z _{ bsv
    }\label{eq_transformer_conditional_prob}
\end{align}
and so the model is trained using the usual cross-entropy/maximum-likelihood loss
\begin{align}
  \Lcal(x, z) &= \frac{ 1 }{ BK }\sum _{ b,s }\ln p(x _{ b (s+1) }=| x _{ b s }, x _{ b (s-1) },
  \ldots, x _{ b 0 })\nn
  &= \frac{ 1 }{ BK }\sum _{ b,s }\Sm _{ v }\ z _{ bsv}\big| _{ v=y _{ bs } } \ .
\end{align}
Note that the losses for all possible context lengths are included in the sum.

In \pyinline{torch}
code, the loss computation might look like the following:
\pyfile[firstline=10, lastline=23]{python/loss.py}

\part{Training}

\section{Memory \label{sec_memory_training}}

In this section we summarize the train-time memory costs of Transformers under various training
strategies\footnote{A nice related blog post is \href{https://blog.eleuther.ai/transformer-math/}{here}.}.

The memory cost is much more than simply the cost of the model
parameters. Significant factors include:
\begin{itemize}
\item Optimizer states, like those of \pyinline{Adam}
\item Mixed precision training costs, due to keeping multiple model copies.
\item Gradients
\item Activation memory\footnote{Activations refers to any intermediate value which needs to be
    cached in order to compute backpropagation. We will be conservative and assume that the inputs
of all operations need to be stored, though in practice gradient checkpointing and recomputation
allow one to trade caching for redundant compute. In particular, flash attention
\cite{dao2022flashattention} makes use of this strategy.} , needed for backpropagation.
\end{itemize}
Because the activation counting is a little more involved, it is in its own section.


\begin{nicebox}{Essentials}
Memory costs count the elements of all tensors in some fashion, both from model parameters and
intermediate representations. The gradient and optimizer state costs scale with the former quantity:
$ \Ocal \left ( N _{ {\rm params}  } \right ) \sim \Ocal \left ( L D ^{ 2 } \right )$, only counting
the dominant contributions from weight matrices. Activation memory scales with the latter,
which for a \pyinline{(B, S, D)}-shaped input gives $ \Ocal \left ( LBSD  \right ) $ contributions
from tensors which preserve the input shape, as well as $ \Ocal \left( LBHS ^{ 2 } \right)  $
factors from attention matrices.
\end{nicebox}


\subsection{No Sharding}

Start with the simplest case where there is no sharding of the model states. Handling the different
parallelism strategies later will be relatively straightforward, as it involves inserting just a few
factors here and there.

\subsubsection{Parameters, Gradients, Optimizer States, and Mixed Precision
\label{sec_params_grads_optim_mem}}


Memory from the bare parameter cost, gradients, and optimizer states are fixed costs independent of
batch size and sequence-length (unlike activation memory), so we discuss them all together here. The
parameter and optimizer costs are also sensitive to whether or not mixed-precision is used, hence we
also address that topic, briefly.  We will assume the use of \pyinline{Adam}\footnote{Which stores
    \href{https://pytorch.org/docs/stable/generated/torch.optim.Adam.html}{two different running
averages} per-model parameter.} throughout, for simplicity and concreteness. It will sometimes be
useful below to let $ p $ to denote the precision in bytes that any given element is stored in, so
\pyinline{torch.float32} corresponds to $ p=4 $, for instance. Ultimately, we primarily consider
vanilla training in $ p=4 $ precision and \pyinline{torch.float32}/\pyinline{torch.float16} ($ p=4
$/ $ p=2 $)  mixed-precision, other, increasingly popular variants to exist, so we keep the
precision variable where we can.


Without mixed precision, the total cost of the
\pyinline{torch.float32} ($ p=4 $ bytes) model and optimizer states in bytes is then
\begin{align}
    M ^{ \rm model} &= 4 N _{ \rm params } \ , \quad M^{ \rm  optim } = 8 N _{ \rm params }
    \quad ({\rm no \ mixed \ precision, } p=4)
    \label{eq_optimizer_states_mem_no_mp}
\end{align}
here, from the previous section, the pure parameter-count of the decoder-only Transformers
architecture is
\begin{align}
    N _{ \rm params } &\approx  (4 + 2E) L D ^{ 2 } \times \left ( 1 + \Ocal \left( \frac{ V }{ DL }
    \right) + \Ocal \left( \frac{ 1 }{ D } \right)  \right ) \ . \label{eq_approx_params_no_sharding}
\end{align}
where the first term comes from the \pyinline{TransformerBlock} weight matrices, the first omitted
subleading correction term is the embedding matrix, and the last comes from biases,
\pyinline{LayerNorm} instances, and other negligible factors. \footnote{For the usual $ E=4 $ case,
    the \pyinline{MLP} layers are twice as costly as the \pyinline{CausalAttention} layers.} The
    optimizer states cost double the model itself.


The situation is more complicated when mixed-precision is used \cite{micikevicius2018mixed}.
The pertinent components of mixed-precision:
\begin{itemize}
\item A half-precision ($ p=2 $ bytes) copy of the model is used to perform the forwards and
    backwards passes
\item A second, "master copy" of the model is also kept with weights in full $ p=4 $ precision
\item The internal \pyinline{Adam} states are kept in full-precision
\end{itemize}
Confusingly, the master copy weights are usually accounted for as part of the optimizer state, in
which case the above is altered to
\begin{align}
    M ^{ \rm model} &= 2 N _{ \rm params } \ , \quad M^{ \rm  optim } = 12 N _{ \rm params }
    \quad ({\rm mixed \ precision}) .
    \label{eq_optimizer_states_mem_mp}
\end{align}
The optimizer state is now six times the cost of the actual model used to process data and the costs
of \eqref{eq_optimizer_states_mem_mp} are more than those of \eqref{eq_optimizer_states_mem_no_mp}.
However, as we will see, the reduced cost of activation memory can offset these increased costs, and
we get the added benefit of increased speed due to specialized hardware. The above also demonstrates
why training is so much more expensive than inference.


\subsubsection{Gradients}

Gradients are pretty simple and always cost the same regardless of whether or not mixed-precision is
used:
\begin{align}
    M ^{ \rm grad} &= 4 N _{ \rm params }     \label{eq_grad_memory} \ .
\end{align}
In mixed precision, even though the gradients are initially computed in $ p= 2$, they
\href{https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one#anatomy-of-models-memory}{have to
be converted} to $ p=4 $ to be applied to the master weights of the same precision.




\subsubsection{Activations}

Activations will require a more extended analysis \cite{korthikanti2022reducing}. Unlike the above
results, the activation memory will depend on both the batch size and input sequence length, $ B $
and $ S $, scaling linearly with both.



\paragraph{Attention Activations}

We will count the number of input elements which need to be cached. Our \pyinline{(B, S, D)}-shaped inputs
to the attention layer with $ BSD $ elements are first converted to $ 3BSD $ total query, key, value
elements, and the query-key dot products produce $ HBS ^{ 2 } $ more, which are softmaxed into $ HBS
^{ 2 } $ normalized scores. The re-weighted inputs to the final linear layer also have $ BSD $
elements, bringing the running sum to $ BS \left ( 5D + 2HS  \right ) $

Finally, there are also the dropout layers applied to the normalized attention scores and the final
output whose masks must be cached in order to
backpropagate. In torch, the mask is a \pyinline{torch.bool} tensor, but
\href{https://github.com/pytorch/pytorch/issues/41571}{surprisingly} these use one \textit{byte} of
memory per element, rather than one bit \footnote{As you can verify
via \pyinline{4 * torch.tensor([True]).element_size() == torch.tensor([1.]).element_size().}}.
Given this, the total memory cost from activations per attention layer is
\begin{align}
M _{ \rm act  } ^\texttt{Attention} &= BS \left ( (5p+1)D + (2p+1)HS  \right ) \ .
\label{eq_att_actmem_vanilla}
\end{align}




\paragraph{MLP Activations}

  Eirst we cache the
\pyinline{(B, S, D)}-shaped inputs into the first MLP layer. These turn into the
\pyinline{(B, S, ED)} inputs of the non-linearity, which are then passed into the last
\pyinline{Linear} layer, summing to $ (2E+1)BSD $ total elements thus far. Adding in the dropout
mask, the total memory requirement is
\begin{align}
M _{ \rm act  } ^\texttt{MLP} &= (2Ep+p+1)BSD\ .
\label{eq_mlp_actmem_vanilla}
\end{align}


\paragraph{LayerNorm, Residual Connections, and Other Contributions}

Then the last remaining components. The \pyinline{LayerNorm} instances each have $ BSD $ inputs and
there are two per transformer block, so $ M _{ {\rm  act}  } ^\texttt{LayerNorm} = 2pBSD $ for each
layer, and there is an additional instance at the end of the architecture. There are two residual
connections per block, but their inputs do not require caching (since they're just additions whose
derivatives are independent of inputs). Then, there are additional contributions from pushing the
last layer's outputs through the language-model head and computing the loss function, but these do
no scale with $ L $ and are ultimately $ \sim \Ocal \left( \frac{ V }{ DL } \right)  $ suppressed,
so we neglect them.





\paragraph{Total Activation Memory}


Summing up the contributions above, the total activation memory cost per-layer is
\begin{align}
    M _{ {\rm act}  } ^{ {\rm  total}  } & \approx  2BDLS   \left ( p(E+4) + 1 + \Ocal \left( \frac{
    V}{ DL } \right)  \right )
    + BHLS ^{ 2 } \left ( 2p+1\right ) \label{eq_act_mem_total_no_sharding}\ .
\end{align}
Evaluating in common limits, we have:
\begin{align}
    M _{ {\rm act}  } ^{ {\rm  total}  } \Big| _{ E=4, p=4 } &=BLS \left ( 66 D+15HS  \right ) \nn
    M _{ {\rm act}  } ^{ {\rm  total}  } \Big| _{ E=4, p=2 } &=BLS \left ( 34 D+5HS  \right )
\end{align}


\paragraph{When does mixed-precision reduce memory?} (Answer: usually.) We saw in Sec.~\ref{sec_params_grads_optim_mem}
that mixed precision \textit{increases} the fixed costs of non-activation memory, but from the above
we also see that it also \textit{reduces} the activation memory and the saving increase with larger
batch sizes and sequence lengths. It is straightforward to find where the tipping point is and for
the simplified $ E=4$, vanilla mixed-precision case with no parallelism the minimum batch size which
leads to memory savings is
\begin{align}
  B _{ {\rm min}  } &= \frac{ 6 D ^{ 2 } }{ 8 DS + H S ^{ 2 } }\label{eq_min_mp_batch_size}\ .
\end{align}
For typical architectures and training sequence lengths, the two terms in the denominator are about
equal and so we can approximate the above as $ B _{ {\rm min}  } \sim \frac{ D }{ S }  \sim \Ocal
\left( 5 \right) $ for typical mid-to-large models in Summer 2023. The constant factors are
important, but they turn out to help us: for GPT-2 and GPT-3

\subsection{Tensor Parallelism \label{subsec_tensor_parallelism} }


In \textbf{Tensor Parallelism}, sometimes also called \textbf{Model Parallelism}, individual weight
matrices are split across devices \cite{shoeybi2020megatronlm}. We consider the \pyinline{MLP} and
\pyinline{CausalAttention} layers in turn. Assume $ T $-way parallelism such that we split some
hidden dimension into $ T $-equal parts. The total number of workers is some $ N \ge T $ which is
evenly divisible by $ T $. With $ N >T $ workers, the workers are partitioned into $ N/T $ groups
and collective communications will be required within, but not across, groups\footnote{Ideally, all
the workers in a group reside on the same node.}. The members of a given group need to all process
the same batch of data; data-parallelism must span different groups.


\begin{nicebox}{Essentials}
The cost of large weights can be amortized by first sharding its output dimension, resulting in
differing activations across group members. Later, the activations are brought back in sync via
an \pyinline{AllReduce}. Weights which act on the sharded-activations can also be sharded in their
input dimension.
\end{nicebox}

\paragraph{MLP}
It is straightforward to find the reasonable ways in which the weights can be partitioned. We
suppress all indices apart from those of the hidden dimension for clarity.

The first matrix multiply $ z _{ d }W ^{ 0 } _{ d e } $ is naturally partitioned across the output
index, which spans the expanded hidden dimension $ e\in \left \{ 0, \ldots , EH-1 \right \} $. This
functions by splitting the weight matrix across its output indices across $ T $ devices:  $ W ^{ 0 }
_{ d e } \longrightarrow W ^{ 0(t) } _{ d e }$, where in the split weights $ t\in \left \{ 0,
    \ldots , T-1  \right \} $, and $ e \in \left \{ t\frac{  EH}{ T } ,
\ldots ,\left ( t+1 \right ) \frac{ EH }{ T } \right \} $. Note that each worker will have to store
all components of the input $ z $ for their backward pass, and an \pyinline{AllReduce} operation
(see App.~\ref{app_collective_communications}) will be needed to collect gradient shards from other
workers.


Let the partial outputs from the previous step again be $ z _{ e } ^{ (t) } $, which are
\pyinline{(B, S, E*H/T)}-shaped. The non-linearity $ \phi $ acts element wise, and using the subsequent
$ z _{ e } ^{ (t) }  $ to compute the second matrix multiply requires a splitting the weights as in $ W ^{ 1 }
_{ e d } \longrightarrow W ^{ 1(t) } _{  e d}$, such that the desired output is computed as in
\begin{align}
     z _{ e }\cdot W ^{ 1 }_{ ed }&=z _{ e }^{ (t) }\cdot W ^{ 1(t) }_{ ed } \ ,
\end{align}
sum over $ t $ implied, with each device computing one term in the sum.  The sum is implemented by
an \pyinline{AllReduce} operation in practice. Note that no \pyinline{AllReduce} is needed for the
backwards pass, however.



\begin{figure}[ht]
 \centering
 \includegraphics[scale=.3]{figures/mlp_mp_2.png}
 \caption{Tensor parallelism for the \pyinline{MLP} layers. Graphic from
 \cite{shoeybi2020megatronlm}. The $ f/g $ operations are the collective
identity/\pyinline{AllReduce} operations in the forwards pass and the \pyinline{AllReduce}/identity
operations in the backwards pass.}
 \label{fig_mlp_tensor_parallel}
\end{figure}


 \paragraph{Attention} The computations for each the $ H $ individual attention heads, which result
 in the various re-weighted values $ t ^{ h }_{ bsf } $ \eqref{eq_reweighted_values}, can be
 partitioned arbitrarily across workers without incurring any collective communications costs.  Each
 worker then holds some subset of these $ h \in \left \{ 0, \ldots , H-1 \right \} $ activations and
 the final output matrix multiply can be schematically broken up as in \begin{align} &{\rm Concat}
     \left ( \left [ t ^{ 0 }, \ldots , t ^{ H-1 } \right ] \right ) \cdot O \nn &= {\rm Concat}
     \left ( \left [ {\rm Concat}\left ( \left [ t ^{ 0 }, \ldots , t ^{ \frac{ H }{ T }-1 } \right
     ] \right ) \cdot O ^{ (0) }, \ldots ,{\rm Concat}\left ( \left [ t ^{ H-\frac{ H }{ T } },
\ldots , t ^{ H} \right ] \right ) \cdot O ^{ (T-1) } \right ] \right ) \ , \end{align} where matrix
products and concatenation both occur along the hidden dimension. That is, each worker in a group
has $ H/T $ different \pyinline{(B, S, D/H)}-shaped activations $ t ^{ h } $, which can be
concatenated into a \pyinline{(B, S, D/T)}-shaped tensor and multiplied into the
\pyinline{(D/T, D)}-shaped shard of $ O $ whose dimensions correspond to those in the just-concatenated tensor.
Concatenating together each such result from every worker (via an \pyinline{AllReduce}) gives the
desired output. The backwards pass requires similar collective communications to the \pyinline{MLP}
case above.


 \begin{figure}[ht]
     \centering
     \includegraphics[scale=.3]{figures/attention_mp_2.png}
     \caption{Tensor parallelism for the \pyinline{CausalAttention} layers. Graphic from
     \cite{shoeybi2020megatronlm}. The $ f/g $ operators play the same role as in
 Fig.~\ref{fig_mlp_tensor_parallel}.}
     \label{fig_attn_tensor_parallel}
 \end{figure}

 \paragraph{Embedding and LM Head} Last, we can apply tensor parallelism to the language model head,
 which will also necessitate sharding the embedding layer, if the two share weights, as typical.

For the LM head, we shard the output dimension as should be now familiar, ending up with $ T $
different \pyinline{(B, S, V/T)}-shaped tensors, one per group member. Rather than communicating
these large tensors around and then computing the cross-entropy loss, it is more efficient to have
each worker compute their own loss where possible and then communicate the scalar losses
around\footnote{In more detail, given the gold-answers $ y _{ bs } $ for the next-token-targets, a
given worker can compute their contribution to the loss whenever their \pyinline{(B, S, V/T)}-shaped
output $ z _{ bsv' } $ contains the vocabulary dimension $ v _{ * } $ specified by $ y _{ bs } $,
otherwise those tensor components are ignored.}.

For a weight-tied embedding layer, the former construction requires \pyinline{AllReduce} in order
for every worker to get the full continuous representation of the input.

\paragraph{LayerNorm and Dropout} \pyinline{LayerNorm} instances are not sharded in pure tensor
parallelism both because there is less gain in sharding them parameter-wise, but also sharding
\pyinline{LayerNorm} in particular would require additional cross-worker communication, which we
wish to reduce as much as possible. \pyinline{Dropout} layers are also not sharded in  where
possible in pure tensor parallelism, but sharding the post-attention \pyinline{Dropout} layer is
unavoidable. It is the goal of sequence parallelism is to shard these layers efficiently; see
Sec.~\ref{subsec_seq_parallelism}.



 \paragraph{Effects on Memory} The per-worker memory savings come from the sharding of the weights
 and the reduced activation memory from sharded intermediate representations.

 The gradient and optimizer state memory cost is proportional to the number of parameters local to
 each worker (later we will also consider sharding these components to reduce redundantly-held
 information). The number of parameters per worker is reduced to
\begin{align}
    N _{ \rm params } &\approx  (4 + 2E) \frac{ L D ^{ 2 } }{ T }\ ,
    \label{eq_approx_params_tensor_parallel}
\end{align}
counting only the dominant contribution from weights which scale with $ L $, since every weight is
sharded. Since all non-activation contributions to training memory scale with $ N _{ {\rm params}  }
$, this is a pure $ 1/T $ improvement.

 The per-layer activation memory costs \eqref{eq_att_actmem_vanilla} and
 \eqref{eq_mlp_actmem_vanilla} are altered to:
\begin{align}
M _{ \rm act  } ^\texttt{Attention} &= BS \left ( \left (p + \frac{ 4p }{ T }+1 \right )D + \left
(\frac{ 2p+1 }{ T } \right )HS  \right ) \nn
M _{ \rm act  } ^\texttt{MLP} &= \left (\frac{ 2Ep }{ T }+p+1 \right )BSD\ .
\label{eq_act_mem_attn_mlp}
\end{align}
The derivation is similar to before. Adding in the (unchanged) contributions from
\pyinline{LayerNorm} instances, the total, leading order activation memory sums to
\begin{align}
    M _{ {\rm act}  } ^{ {\rm  total}  } & \approx  2BDLS   \left ( p \left (2+ \frac{ E+2 }{ T }\right ) + 1   \right )
    + BHLS ^{ 2 } \left ( \frac{ 2p+1 }{ T }\right ) \label{eq_act_mem_total_tensor_parallel}\ .
\end{align}
Again, the terms which did not receive the $ 1/T $ enhancement correspond to activations from
unsharded \pyinline{LayerNorm} and \pyinline{Dropout} instances and the $ 1/T $'s improvements can
be enacted by layering sequence parallelism on top (Sec.~\ref{subsec_seq_parallelism}).


\subsection{Sequence Parallelism \label{subsec_seq_parallelism}}

In \eqref{eq_act_mem_total_tensor_parallel}, not every factor is reduced by $ T $.
\textbf{Sequence Parallelism} fixes that by noting that the remaining contributions, which
essentially come from \pyinline{Dropout} and \pyinline{LayerNorm}, can be parallelized in the sequence dimension (as can
the residual connections).

The collective communications change a bit. If we shard the tensors across the sequence dimension
before the first \pyinline{LayerNorm}, then we want the following:
\begin{enumerate}
\item The sequence dimension must be restored for the \pyinline{CausalAttention} layer
\item The sequence should be re-split along the sequence dimension for the next \pyinline{LayerNorm} instance
\item The sequence dimension should be restored for the \pyinline{MLP} layer \footnote{This doesn't
    seem like a hard-requirement, but it's what is done in \cite{korthikanti2022reducing}.}
\end{enumerate}

The easiest way to achieve the above is the following.
\begin{enumerate}
    \item If the tensor parallelization degree is $ T $, we also use sequence parallelization degree $ T
        $.
    \item The outputs of the first \pyinline{LayerNorm} are \pyinline{AllGather}-ed to form the full-dimension
        inputs to the \pyinline{CausalAttention}  layer
    \item The tensor-parallel \pyinline{CausalAttention} layer functions much like in
        Fig.~\ref{fig_attn_tensor_parallel} \textit{except} that we do not re-form the outputs to
        full-dimensionality.  Instead, before the \pyinline{Dropout} layer, we \pyinline{ReduceScatter} them
        from being hidden-sharded to sequence-sharded and pass them through the subsequent
        \pyinline{Dropout}/\pyinline{LayerNorm} combination, similar to the first step
    \item The now-sequence-sharded tensors are reformed with another \pyinline{AllGather} to be the full-dimensionality inputs to the
        \pyinline{MLP} layer whose final outputs are similarly \pyinline{ReduceScatter}-ed to be
        sequence-sharded and are recombined with the residual stream
\end{enumerate}
The above allows the \pyinline{Dropout} mask and \pyinline{LayerNorm} weights to be sharded $ T
$-ways, but if we save the full inputs to the \pyinline{CausalAttention} and \pyinline{MLP}  layers
for the backwards pass, their contributions to the activation memory are not reduced (the $ p
$-dependent terms in \eqref{eq_act_mem_attn_mlp}). In \cite{korthikanti2022reducing}, they solve
this by only saving a $ 1/T $ shard of these inputs on each device during the forward pass and then
performing an extra \pyinline{AllGather} when needed during the backwards pass. Schematics can be
sen in Fig.~\ref{fig_tensor_seq_parallel} and Fig.~\ref{fig_tensor_seq_parallel_detail} below.

\begin{figure}[ht]
 \centering
 \includegraphics[scale=.25]{figures/transformer-tensor-sequence-parallel.jpg}
 \caption{Interleaved sequence and tensor parallel sections. Graphic from
 \cite{shoeybi2020megatronlm}. }
 \label{fig_tensor_seq_parallel}
\end{figure}

\begin{figure}[ht]
 \centering
 \includegraphics[scale=.25]{figures/mlp-tensor-sequence-parallel.jpg}
 \caption{Detail of the sequence-tensor parallel transition for the \pyinline{MLP} . Graphic from
 \cite{shoeybi2020megatronlm}. }
 \label{fig_tensor_seq_parallel_detail}
\end{figure}


\subsection{Pipeline Parallelism \label{subsec_pipe_parallelism}}






\subsection{Case Study: Mixed-Precision GPT3 \label{subsec_gpt_mem_study} }

Let's run through the numbers for mixed-precision GPT3 with
\href{https://bmk.sh/2020/05/29/GPT-3-A-Brief-Summary/}{parameters}:
\begin{align}
L &= 96 \ , \quad
D = 12288 \ ,\quad
H = 96\ , \quad V = 50257\ .
 \label{eq_gpt_num}
\end{align}
We are leaving the sequence-length unspecified, but the block-size (maximum sequence-length) is $
K=2048 $.


Start by assuming no parallelism at all. The total (not per-layer!) non-activation memory is
\begin{align}
  M _{ {\rm non-act}  } ^ \texttt{GPT-3} & \approx 1463\ {\rm TiB}
\end{align}
which can be broken down further as
\begin{align}
  M _{ {\rm params}  } ^ \texttt{GPT-3} & \approx 162\ {\rm TiB} \ , \quad
 M _{ {\rm grads}  } ^ \texttt{GPT-3}  \approx 325\ {\rm TiB}\ , \quad
  M _{ {\rm optim}  } ^ \texttt{GPT-3}  \approx 975\ {\rm TiB}\ .
\end{align}
The embedding matrix only makes up $ \approx .3\% $ of the total number of parameters, justifying our
neglect of its contribution in preceding expressions.


The activation memory is
\begin{align}
  M _{ {\rm act}  } ^ \texttt{GPT-3} & \approx 3 \times 10 ^{ -2 }BS\times  \left (  1  + \frac{ S
  }{ 10 ^{ 3 } } \right ) \ {\rm TiB} \ .
\end{align}
Note that the attention matrices, which are responsible for $ \Ocal \left( S ^{ 2 } \right)  $ term, will
provide the dominant contribution to activation memory in the usual $ S \gtrsim 10 ^{ 3 } $ regime.

In the limit where we process the max block size ($ S=K=2048 $), the ratio of activation to
non-activation memory is
\begin{align}
  \frac{  M _{ {\rm act}  } ^ \texttt{GPT-3}}{ M _{ {\rm non-act}  } ^ \texttt{GPT-3} }\Big| _{
  S=2048 } & \approx  .2 B \ .
\end{align}
So, the activation memory is very significant for such models.


Using tensor parallelism into the above with the maximal $ T=8 $ which can be practically used, the
savings are significant. The total memory is now
\begin{align}
  M _{ {\rm total}  } ^{ \texttt{GPT-3}  } & \approx 187\ {\rm TiB} + 10 ^{ -2 }BS + 5 \times 10 ^{
  -6} BS ^{ 2 }\ .
\end{align}




\section{Training FLOPs \label{sec_flops} }

The total number of floating point operations (FLOPs)\footnote{The notation surrounding
floating-point operations is very confusing because another quantity of interest is the number
of floating-point operations a given implementation can use \textit{per-second}. Sometimes,
people use FLOPS or FLOP/s to indicate the rate, rather than the gross-count which has the lower
case ``s", FLOPs, but there's little consistency in general.}  needed to process a given batch of
data is effectively determined by the number of matrix multiplies needed.

Recall that a dot-product of the form $ v \cdot M $  with $ v \in \mathbb{R}^{ m } $ and $ M \in
\mathbb{R} ^{ m, n }$ requires $ m\times \left (2 n-1 \right ) \approx 2mn$ FLOPs.  For large
language models, $ m,n \sim \Ocal \left( 10 ^{ 3 } \right)  $, meaning that even expensive
element-wise operations like \pyinline{GeLU} acting on the same vector $ v $ pale in comparison by
FLOPs count \footnote{Since their FLOPs counts only scales as $ \sim \Ocal \left( n\right )  $ where
the omitted constant may be relatively large, but is still not $ \Ocal \left( n \right)  $.}. It is
then a straightforward exercise in counting to estimate the FLOPs for a given architecture. The input tensor
is of shape \pyinline{(B, S, D)} throughout.



\subsection{No Recomputation}

Start with the case where there is no recomputation activations.  These are the \textbf{model FLOPs} of
\cite{korthikanti2022reducing}, as compared to the \textbf{hardware FLOPs} which account for gradient
checkpointing.


\paragraph{\pyinline{CausalAttention}: Forwards }

The FLOPs costs:
\begin{itemize}
    \item  Generating the query, key, and value vectors: $ 6BSD ^{ 2 } $
    \item Attention scores:  $2BDS ^{ 2 }$
    \item Re-weighting values:  $2BDS ^{ 2 }$
    \item Final projection: $ 2BSD ^{ 2 } $
\end{itemize}

\paragraph{\pyinline{MLP}: Forwards}
Passing a  through the \pyinline{MLP} layer, the FLOPs due to the
first and second matrix-multiplies are equal, with total matrix-multiply requires $ 4BSED ^{ 2 } $.

\paragraph{Backwards Pass}

We estimate the backwards pass as costing twice the flops as the forwards pass. This estimate comes
from just counting the number of $ \Ocal \left( n ^{ 2 } \right)  $ operations, (i.e.
matrix-multiplies and outer-products) which dominate the FLOPs computation.  The argument is the
standard one: in the backwards pass we are given the upstream derivative $ \partial _{ z '  } \Lcal
$ where $ z' = \phi \left ( W \cdot z \right ) $ for some non-linearity $ \phi $, where the
weight-matrix multiply is the costly operation whose FLOPs we have accounted for in the forward
pass. Here, $ z $ is a tensor of some arbitrary shape and the matrix multiply is over one of its
dimensions, in the usual way. Without loss of generality, let's be more specific and let $ z $ be
\pyinline{(d0, ... , dn, D)}-shaped with $N= \prod _{ i } d_{i  } $ total elements let $ W $ be
\pyinline{(D, E)}-shaped such that it acts on the last index of $ z $, making $ z' $
\pyinline{(d0, ... , dn, E)}-shaped. The forward-FLOPs cost of this operation is therefore $ 2NE $.

In order to complete backpropagation, we need both to compute $ \partial  _{ W }\Lcal  $ to update $
W $ and also $ \partial  _{ z } \Lcal  $ to continue backpropagation to the next layer down. Each of
these computations will be about as costly as the forward computation, hence the claim.

Schematically, the \pyinline{(D, E)}-shaped weight derivative is
\begin{align}
    \partial  _{ W }\Lcal  &= \partial _{ z '  } \Lcal \cdot \phi' \left ( W \cdot z \right ) \cdot
z\ .
\end{align}
On the right size, $ z $ and $ W \cdot  z $ are cached and the element-wise computation of $ \phi'
\left ( W \cdot z \right ) $ is negligible, as discussed above. The bulk of the computation then comes from
taking the three factors, which are \pyinline{(d0, ... , dn, E)}-shaped,
\pyinline{(d0, ... , dn, D,  E)}-shaped, and \pyinline{(d0, ... , dn, D)}-shaped, respectively,
multiplying and summing over all dimensions other than the \pyinline{D}-  and \pyinline{E}-shaped
ones. This also requires $ \sim 2ND' $ FLOPs, the same as the forward pass.


And similarly, the \pyinline{(d0, ... , dn, D)}-shaped activation derivative is
\begin{align}
    \partial  _{ z }\Lcal  &= \partial _{ z '  } \Lcal \cdot \phi' \left ( W \cdot z \right )  \cdot  W
\end{align}
and a nearly identical argument to the above gives a second $ \sim  2ND'  $ contribution to the
backwards FLOPs count.

Hence, the number of matrix-multiply-like operations in the backwards pass is approximately double that
of the forward pass, justifying the argument.





\paragraph{Total Model Flops}


The grand sum is then\footnote{With a large vocabulary, the cost of the final language model head
matrix multiply can also be significant, but we have omitted its $ L $-independent,  $ 2BSDV $
contribution here. }:
\begin{align}
F _{ {\rm total}  } ^{ {\rm  model}  } &\approx 12 BDLS \left ( S + \left ( 2+E \right )D \right ) \label{eq_model_flops}\ .
\end{align}
We can also phrase the flops in terms of the number of parameters as
\begin{align}
 F _{ {\rm total}  } ^{ {\rm  model}  } \big| _{ T=1 }  &= 6BS N _{ {\rm  params}  }\times \left ( 1 + \Ocal \left( S/D\right)  \right )
\end{align}
where we took the $ T=1, D \gg S $ limit for simplicity.


\part{Inference}







\appendix


\section{Conventions and Notation}\label{app_conventions}


We loosely follow the conventions of \cite{korthikanti2022reducing} and denote the main Transformers
parameters by:
\begin{itemize}
\item $ B $: microbatch size
\item $ K $: the block size (maximum sequence length\footnote{In the absence of methods such as
    ALiBi \cite{ALiBi}  can be used to extend the sequence length at inference time.})
\item $ S $: input sequence length
\item $ V $: vocabulary size
\item $ D $: the hidden dimension size
\item $ L $: number of transformer layers
\item $ H $: number of attention heads
\item $ P $: pipeline parallel size
\item $ T $: tensor parallel size
\item $ E $: expansion factor for MLP layer (usually $ E=4 $)
\end{itemize}
Where it makes sense, we try to use the lower-case versions of these characters to denote the
corresponding indices on various tensors. For instance, an input tensor with the above batch size,
sequence length, and vocabulary size would be written as $ x _{ bsv } $, with $ b \in \left \{ 0,
\ldots, B - 1 \right \} $, $ s \in \left \{ 0, \ldots, S - 1\right \} $, and $  v \in \left \{ 0,
    \ldots, V -1\right \}$ in math notation, or as \mintinline{python}{x[b, s, v]} in code.  Typical
    transformers belong to the regime
\begin{gather}
V \gg D, S \gg L, H \gg P, T \ .  \label{app_eq_transformers_approxs}
\end{gather}
For instance, GPT-2 and GPT-3 \cite{gpt2radford2019language, gpt3brown2020language} have $ V \sim \Ocal \left( 10 ^{ 4 } \right)  $,
$ S, L \sim \Ocal \left( 10 ^{ 3 } \right)  $, $ L, H \sim \Ocal \left( 10 ^{ 2 } \right)  $.


As indicated above,  we use zero-indexing. We also use \pyinline{python} code
throughout\footnote{Written in a style conducive to latex, e.g. no type-hints and pegagogy
prioritized.}  and
write all ML code using standard \pyinline{torch} syntax. To avoid needing to come up with new
symbols in math expressions we will often use expressions like $ x \leftarrow f(x) $ to refer to performing
a computation on some argument ($ x $) and assigning the result right back to the variable $ x $
again.

Physicists often joke (half-seriously) that Einstein's greatest contribution to physics was his
summation notation in which index-sums are implied by the presence of repeated indices and summation
symbols are entirely omitted. For instance, the dot product between two vectors would be written as
\begin{align}
    \vec{x} \cdot \vec{y} &= \sum _{ i } x _{ i } y _{ i } \equiv x _{ i } y _{  i }
    \label{app_eq_einstein_sum}
\end{align}
We use similar notation which is further adapted to the common element-wise deep-learning
operations.  The general rule is that if a repeated index appears on one side of an equation, but
not the other, then a sum is implied, but if the same index appears on both sides, then it's an
element-wise operation. The Hadamard-product between two matrices $ A $ and $ B $ is just
\begin{align}
  C _{ ij } &= A _{ ij } B _{ ij }\ .
\end{align}
Einstein notation also has implementations available for \pyinline{torch}:
\href{https://rockt.github.io/2018/04/30/einsum}{see this blog post on \pyinline{einsum}} or the
\href{https://einops.rocks/1-einops-basics/}{\pyinline{einops}} package.

 We also put explicit indices on operators such as Softmax to help clarify the relevant
 dimension, e.g. we would write the softmax operation over the $ b $-index of some batched
 tensor $ x _{ bvd\ldots } $ as
 \begin{align}
     s _{ bvd\ldots } &= \frac{ e^{ x _{ bv d\ldots}  } }{ \sum _{ v = 0 } ^{  v= V-1 } e^{ x _{
     bvd\ldots } } } \equiv
     \Sm _{ v } \ x _{ bvd\ldots }
     \ , \label{app_eq_einstein_softmax}
 \end{align}
 indicating that the sum over the singled-out $ v $-index is gives unity.

\section{Collective Communications \label{app_collective_communications} }

A quick refresher on common distributed
\href{https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html}{communication
primitives}.  Consider $ N $ workers with tensor data $ x ^{ (n) }  $ of some arbitrary shape
\pyinline{x.shape}, where $ n $ labels the worker and any indices on the data are suppressed. The $
n=0 $ worker is arbitrarily denoted the \textit{chief}.  Then, the primitive operations are:
\begin{itemize}
    \item \pyinline{Broadcast}: all workers receive  the chief's data, $ x ^{ (0) }  $.
    \item \pyinline{Gather}: all workers communicate their data $ x _{ n } $ to the chief, e.g. in a
        concatenated array $ [x ^{ 0 }, x ^{ 1 }, \ldots , x ^{ N-1 }] $.
    \item \pyinline{Reduce}: data is \pyinline{Gather}-ed to the chief, which then performs some
        operation (\pyinline{sum}, \pyinline{max}, \pyinline{concatenate}, etc.) producing a new tensor $
        x' $ on the chief worker.
    \item \pyinline{AllGather}: \pyinline{Gather} followed by \pyinline{Broadcast}, such that all
        data $ x ^{ (n) } $ is communicated to all workers.
    \item \pyinline{AllReduce}: generalization of \pyinline{Reduce} where all
        workers receive the same tensor $ x' $ produced by operating on the $ x ^{ (n) } $.
        Equivalent to a \pyinline{Reduce} followed by \pyinline{Broadcast}, or a
        \pyinline{ReduceScatter} followed by a \pyinline{AllGather} (the more efficient
        choice\footnote{The former strategy scales linearly with the number of worker, while the
            latter strategy underlies ``ring" \pyinline{AllReduce} which is (nearly) independent of
            the number of workers.
            \href{https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/}{See this blog
                post for a nice visualization} or \cite{bandwidthOptimalAllReduce2009} for a
            relevant paper.\label{foot_all_reduce}}).
    \item \pyinline{ReduceScatter}: a reducing operation is applied to the $ x ^{ (n) } $ to produce
        a $ x' $ of the same shape, but each worker only receives a slice $ 1/N $ of the result.
\end{itemize}



 \section{TODO}


 \begin{itemize}
 \item Tokenizers
 \item Generation
 \item Activations
 \item Flash attention
 \item BERT family
 \item Residual stream
 \item Scaling laws
 \end{itemize}

\bibliographystyle{latex_goon/utphys}
\bibliography{bibliography}
\end{document}




