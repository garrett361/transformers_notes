\part{Inference}


\section{Basics and Problems}


The essentials of decoder-only inference is that a given input sequence $ x _{ bs } $ is turned into
a probability distribution $ p _{ bsv } $ over the vocabulary for what the next token might be.  Text
is then generated by sampling from $ p _{ bsv } $ in some way, appending that value to $ x _{ bs } $
to create a one-token-longer sequence, and then repeating until desired.

There are various problems that naive implementations of the above face:
\begin{itemize}
	\item Repeated computation from processing the same tokens in the same order repeatedly, at least for
	      some sub-slice of $ x _{ bs } $.
	\item Inherently sequential computation, rather than parallel
	\item Sub-optimal sampling strategies. Just choosing the most-probably token at each new step, does
	      not guarantee the most-probable overall sequence, for instance.
\end{itemize}



\section{Generation Strategies \label{sec_generation_strats} }

A quick tour of generation strategies. A very readable blog post comparing strategies can be
\href{https://huggingface.co/blog/how-to-generate}{found here.}


\subsection{Greedy \label{subsec_greedy_gen}}

The most obvious generation strategy is to take the final, \pyinline{(B, S, V)}-shaped outputs $ z
		_{ bsv } $ and just take the next token to be the most-probable one (for the final position in the
sequence): \pyinline{next_token = z[:, -1].argmax(dim=-1)}. A very minimal \pyinline{generate} method
is as below:
\pyfile[firstline=6, lastline=23]{python/greedy_generate.py}

There are various important, practical considerations which are ignored in the above implementation, including:
\begin{itemize}
	\item Since we are taking the prediction from the last (\pyinline{-1}-indexed) element in each
	      sequence, it is crucial that all padding is \textit{left}-padding, so that these final
	      elements are meaningful.
	\item Models will signal the end of generation by outputting tokenizer-specific codes, and
	      generation must respect these.
\end{itemize}
See
\href{https://github.com/huggingface/transformers/blob/04ab5605fbb4ef207b10bf2772d88c53fc242e83/src/transformers/generation/utils.py#L1115}{the
	\texttt{generate} method from the \texttt{transformers} library} for more fully-featured code
(which, correspondingly, is not always easy to follow).

\subsection{Simple Sampling: Temperature, Top-$ k $, and Top-$ p $ \label{subsec_simple_sampling}}

The next-most-obvious strategy is to choose the next token by drawing from the probability
distribution defined by the $ z _{ bsv } $. There are various refinements of this idea.

A one-parameter generalization of this strategy
introduces a (physics-motivated) \textbf{Temperature} which just adjusts the scale of the logits:
\begin{py}
	next_token = torch.multinomial((z[:, -1] / temp).softmax(dim=-1), num_samples=1)
\end{py}
assuming \pyinline{z} are the final logits. Larger temperature yields a larger variance in the chosen
tokens.

With temperature sampling, there is still a non-zero chance of choosing an extremely improbable token,
which is undesirable if you do not trust the tails of the distribution. Two common truncation
strategies which guard against this:
\begin{itemize}
	\item \textbf{Top-}$ k $: Only choose from the top-$ k $ most-probable examples (re-normalizing
	      the probabilities across those $ k $ samples)
	\item \textbf{Top-}$  p$: Only choose from the top-however-many most-probable examples whose
	      probabilities sum to $ p $ (again re-normalizing probabilities). This is also sometimes called
	      \textbf{nucleus sampling}.
\end{itemize}


\subsection{Beam Search \label{subsec_beam_search}}

Choosing, say, the most-probable next-token at each step is not guaranteed to yield the most
probable \textit{sequence} of tokens. So, \textbf{Beam Search} explores multiple sequences, using
different branching strategies, and the probabilities of the various beam sequences can be compared
at the end. Important note: generating the most-probable text is not necessarily equal to the most
human-like text \cite{holtzman2020curious}.



\subsection{Speculative Decoding \label{subsec_speculative_decoding}}

Speculative decoding \cite{leviathan2023fastinferencetransformersspeculative} is an excellent idea:
use a cheaper "draft" model to perform the slow, iterative generation steps and check its work with
the full model. Using a detailed-balance-like construction, it can be guaranteed that this
speculative decoding generation strategy creates text drawn from the same distribution as the full
model.

Informally, the algorithm is:
\begin{enumerate}
    \item Generate $ \gamma  $ tokens with the draft model, whose distribution is $ q(x _{ t }|x _{
        {\rm  prefix} }) $, $ t \in \left \{ 0, \ldots , \gamma -1 \right \} $. Write the generated
        tokens as $ z _{ t } $.
    \item Pass the prefix and all $ \gamma  $ generated tokens $ z _{ t } $ through the full model,
        which computes probabilities via its distribution $ p(x _{ t }|x _{ {\rm  prefix} }) $.
    \item For every generated token $ z _{ t } $, accept it
        unconditionally if $ q(z_t| x _{ {\rm  prefix} })\le p(x_t| z _{ {\rm  prefix} })  $. If $
         q(z_t| x _{ {\rm  prefix} }) > p(z_t| x _{ {\rm  prefix} })$, instead accept the token
        with only probability $ \frac{ p }{ q } $.
    \item If only the first $ n < \gamma  $ tokens are accepted, generate token $ n+1 $ from a
        modified distribution $ p'(x_t| x _{ {\rm prefix} }) = F \left [ p(x), q(x) \right ]$ built
        from the two model predictions and chosen (as will be shown) such that the entire algorithm
        generates the correct distribution. $ n+1 $ tokens are created in this case\footnote{We
        cannot generate more because drawing from $ p' $ effectively changes the prefix that the
        full model should use.}.
    \item If all of the $ \gamma  $ tokens are accepted, generate token $ \gamma +1 $ from the full
        model's outputs.
\end{enumerate}

Proof of correctness and the derivation of $ p'(x) $: let $ Q(x_t| x _{ {\rm  prefix}  }) $ be
the distribution described above. Then this can be broken down according to conditioning on the
draft token and whether or not the draft token was accepted. Dropping the prefix condition for
brevity and $ A $ and $ R $ stand for rejected and accepted, respectively, we have
\begin{align}
    Q(x_t) &=\sum _{ z _{ t } } Q(x_t|z _{ t } , A)P(A|z _{ t }) q(z _{ t }) + Q(x_t|z _{ t } , A)P(R|z _{ t }) q(z _{ t }) \nn
         &=\sum _{ z _{ t } } 1 _{ x _{ t }= z _{  t } } \times\min \left (1, \frac{ p(z _{ t }) }{ q( z _{ t })
         }\right ) \times q (z _{  t })  + p'(x _{ t })\left (1 - \min \left (1, \frac{ p(z _{ t }) }{ q( z _{ t })
         }\right )  \right) \times q(z _{ t }) \nn
         &= \min \left (q(x _{ t }),  p(x _{ t })\right ) + p'(x _{ t })\sum _{ z _{ t } }\left (1 - \min \left (1, \frac{ p(z _{ t }) }{ q( z _{ t })
         }\right )  \right) \times q(z _{ t })
\end{align}
The sum is just some constant (denoted by $ 1 - \beta  $ in the paper) and so choosing
\begin{align}
    p'(x _{ t }| {\rm prefix}) &\equiv \frac{ p(x _{ t }| {\rm  prefix}) - \min \left ( q(x _{ t }| {\rm  prefix}), p(x _{ t }| {\rm  prefix}) \right ) }{ 1- \beta }
\end{align}
achieves the goal of getting $Q(x _{ t }| {\rm  prefix}) = p(x _{ t }| {\rm prefix}) $. It can be
verified that this distribution is properly normalized.


\section{The Bare Minimum and the kv-Cache \label{sec_kv_cache}}


There are two separate stages during generation. First, an original, to-be-continued series of prompts
$ x _{ bs }  $ can be processed in parallel to both generate the first prediction and populate any
intermediate values we may want to cache for later. We follow \cite{pope2022efficiently} and call this the
\textbf{prefill} stage. For this procedure, we require the entire $ x _{ bs } $ tensor.

In the second, iterative part of generation (the \textbf{decode} stage) we have now appended
one-or-more tokens to the sequence and we again want the next prediction, i.e. \pyinline{z[:, -1, :]}
for the last-layer outputs $ z _{ bsd } $. In this stage, we can avoid re-processing the entire $ x
		_{ bs } $ tensor and get away with only processing the final, newly added token, \textit{if} we are
clever and cache old results (and accept a very reasonable approximation).

The important pieces occur in the \pyinline{CausalAttention} layer, as that's the only location in
which the sequence index is not completely parallelized across operations. Referring back to
Sec.~\ref{subsubsec_attn_layer}, given the input $ z _{ bsd } $ of the \pyinline{CausalAttention}
layer, the re-weighted value vectors\footnote{Summed over $ s' $, but concatenating the different $
		a $ values over the $ f $ dimension.} $ w ^{ a }_{ bss'd } v ^{ a } _{ bs'f } $ are the key objects
which determine the next-token-prediction, which only depends on the $ s=-1 $ index values.
Therefore, we can cut out many steps and the minimum requirements are:
\begin{itemize}
	\item Only the attention weights $ w ^{ a }_{ bss'd }$ with $ s=-1 $ are needed
	\item The only query values $ q ^{ a }_{ bsd } $ needed to get the above are those with $ s=-1 $
	\item Every component of the key and value vectors $k ^{ a }_{ bsd }, v ^{ a }_{ bsd } $ is
          needed, but because of the causal mask, all components except for the last in the sequence
          dimension ($ s\neq -1 $) are the same as they were in the last iteration, up to a shift by
          one position\footnote{This is where we need to accept a mild approximation, if using a
              sliding attention window. With an infinite context window, if we add a label $ t $
              which indexes the iteration of generation we are on, then we would have that $ z ^{
              (t+1) } _{ bsd } = z ^{ (t)} _{ b (s-1)d } $ for every tensor in the network, except
              for when $ s=-1 $, the last position. The finiteness of the context window makes this
              statement slightly inaccurate because we can only ever keep $ K $ positions in context
              and the loss of the early tokens upon sliding the window over will slightly change the
          values in the residual stream.}
\end{itemize}

So, we are led to the concept of the \textbf{kv-cache} in which we cache old key and query vectors for generation.
The cache represents a tradeoff: fewer FLOPs are needed for inference, but the memory costs are potentially
enormous, since the size of the cache grows with batch size and sequence length:
\begin{align}
	M _{ {\rm kv-cache}  } & = 2pBDLS /T\ ,  \label{eq_kv_cache_memory}
\end{align}
in the general case with tensor-parallelism. This can easily be larger than the memory costs of the
model parameter: $ M _{ {\rm params}  } ^{ {\rm  inference}  } \sim p N _{ {\rm params}  } \sim p LD
^{ 2 }  $ (dropping $ \Ocal \left( 1 \right)  $ factors), so that the cache takes up more memory
when $ BS \gtrsim D $, i.e. when the total number of token exceeds the hidden dimension. Using the
kv-cache eliminates a would-be $ \Ocal \left( S ^{ 2 } \right)  $ factor in the FLOPs needed to
compute a new token, reducing it to linear-in-$ S $ dependence everywhere.

A very minimal implementation\footnote{Warning: very non-optimized code! Purely pedagogical.} is below:
\pyfile[firstline=6, lastline=37]{python/causal_attention_kv_cache.py}


\section{Basic Memory, FLOPs, Communication, and Latency}

The essentials of inference-time math, much of it based on \cite{kipply_inference_math}.

\paragraph{Naive Inference} Processing a single \pyinline{(B, S, D)}-shaped tensor to generate a
single next input costs the $ 2BSN _{ {\rm params}  } $ FLOPs we found for the forwards-pass in
Sec.~\ref{sec_flops_training} (assuming $ S \lesssim D $). Memory costs just come from the parameters
themselves: $ M _{ {\rm infer}  }^{ {\rm naive}  }=pN _{ {\rm params}  } $. Per the analysis of
App.~\ref{app_compute_mem_bound}, naive inference is generally compute-bound and so the
per-token-latency is approximately\footnote{Assuming we do the naive thing here and generate the
    next token in a similarly naive way, shifting over the context window.}  $  2BSN _{ {\rm params}
    }/ \lambda _{ {\rm FLOP/s}  } $ where the FLOPs bandwidth in the denominator is again defined in
    App.~\ref{app_compute_mem_bound}.

\paragraph{kv-Cache Inference}
The FLOPs requirements for the hidden-dimension matrix multiplies during generation are $2BN _{ {\rm params}  } $,
since we are only processing a single token, per previous results.   This is in addition to the up-front cost of $ 2BSN _{
			{\rm params}} $ for the prefill. But, the memory requirements are raised to
\begin{align}
	M _{ {\rm infer}  }^{ {\rm kv-cache}  } & =pN _{ {\rm params}  } + 2pBDLS/T\ .
\end{align}
Inference now has a computational-intensity of
\begin{align}
	\frac{ C _{ {\rm infer} } ^{ {\rm kv-cache} } }{ M _{ {\rm infer}  }^{ {\rm kv-cache}  } } & \sim \frac{ BD }{ S } \ ,
\end{align}
dropping $ \Ocal \left( 1 \right)  $ factors, is now memory-bound (again, see
App.~\ref{app_compute_mem_bound}), and has per-token-latency of approximately $ M _{ {\rm infer} }/
	\lambda _{ {\rm mem} }$, unless the batch-size is very large.


\paragraph{Intra-Node Communication} For $ T $-way tensor parallelism, two \pyinline{AllReduce}s are
needed, one for each \pyinline{MLP} and each \pyinline{CausalAttention} layer, where each
accelerator is sending $ pBDS  $ bytes of data (see Sec.~\ref{subsec_tensor_parallelism}). This
requires a total of $ 4\left ( T-1 \right ) pBDS/T \approx 4pBDS $ bytes to be transferred between
workers in the tensor-parallel group (see Foot.~\ref{foot_all_reduce}), taking a total of $ \sim  4pBDLS/
	\lambda _{ {\rm comms} }  $ time for the model as a whole. For an A100 80GiB, \pyinline{torch.float16} setup, this is $ \sim
	BDS \times  10 ^{ -11 } \ {\rm sec} $


\paragraph{Latency} TODO



\section{Case Study: \href{https://huggingface.co/tiiuae/falcon-40b-instruct?_sm_vck=j230jZ2ssDkkPfJTfRt6tjQNTQZJ65N7VDWmj5Ff6f3jZ3mhh2Pq}{Falcon-40B}}

Let's work through the details of the kv-cache for Falcon-40B\footnote{Falcon actually uses
	multi-query attention, which changes the computations here, but we will pretend it does not in this
	section for simplicity.} with $ D=8192 $, $ L=60 $, $ S=2048 $.  In half, $ p=2 $ precision, the model weights just about fit on an
80GiB A100, but this leaves no room for the cache, so we parallelize $ T $ ways across $ T $ GPUs,
assumed to be on the same node. The total memory costs are then
\begin{align}
	M _{ {\rm  total} } & \approx  \frac{ {\rm 80GiB} + {\rm 4GiB}\times B }{ T } \ .
\end{align}
This means that in order to hit the compute-bound threshold of $ B \sim 200 $ (see
App.~\ref{app_compute_mem_bound}) we need at least $ T=4 $ way parallelism.  Taking $ T=4 $, and
running at capacity with $ B \sim 200$ so that we are compute-bound, the per-iteration latency from
computation alone is approximately $ \frac{2BN _{ {\rm params} } }{ \lambda _{ {\rm FLOP/s} } T} \sim
$13ms, i.e. we can give $ \sim $200 customers about $ \sim $75 tokens-per-second at this
rate\footnote{Average human reading speed is about $ \sim 185$ words/minute, or $ \sim 4
	$tokens/sec.}, if this were the only latency consideration.

